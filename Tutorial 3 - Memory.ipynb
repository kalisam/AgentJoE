{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fa1e388-b8f4-4fa3-be5a-ba7c6caae038",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tutorial 3 - Memory\n",
    "\n",
    "## Key Philosophy\n",
    "- It would be important to learn from past experience and improve the agentic framework - memory is key to that\n",
    "- You can add to the memory bank of your Agents pre-inference (by collecting from a pool of data prior to running the Agent), or during inference (add on in between running subtasks)\n",
    "\n",
    "- In general, for the various Memory classes, you use:\n",
    "    - `append(list_of_memories)`: append a list of memories to the memory bank\n",
    "    - `retrieve(task: str)`: retrieves top_k memories based on task \n",
    "    \n",
    "## Use Memory in Agents\n",
    "- Agent class takes `memory_bank` as a parameter during initialisation of an `Agent`\n",
    "- memory_bank: class Dict[Memory]. Stores multiple types of memory for use by the agent. Customise the Memory config within the Memory class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e38089",
   "metadata": {},
   "source": [
    "# Setup Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cf10ed",
   "metadata": {},
   "source": [
    "## Step 1: Install AgentJo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e017dd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install agentjo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae7b95a",
   "metadata": {},
   "source": [
    "## Step 2: Import required functions and setup relevant API keys for your LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707028fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up API key and do the necessary imports\n",
    "from agentjo import *\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5953438",
   "metadata": {},
   "source": [
    "## Step 3: Define your own LLM\n",
    "- Take in a `system_prompt`, `user_prompt`, and outputs llm response string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d49dcf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(system_prompt: str, user_prompt: str) -> str:\n",
    "    ''' Here, we use OpenAI for illustration, you can change it to your own LLM '''\n",
    "    # ensure your LLM imports are all within this function\n",
    "    from openai import OpenAI\n",
    "    \n",
    "    # define your own LLM here\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        temperature = 0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c013f7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neutral'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that llm function is working\n",
    "llm(system_prompt = 'You are a classifier to classify the sentiment of a sentence', \n",
    "    user_prompt = 'It is a hot and sunny day')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04eacda-4d82-4187-a93e-dd770c95bedc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Memory Class 1: Base Memory Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a176e3-fb39-47f2-910e-eb051b4ead77",
   "metadata": {
    "tags": []
   },
   "source": [
    "- In-house Vector DB that can store anything in a Python list for retrieval to augment the Agent's prompt\n",
    "- Retrieves top k memory items based on task \n",
    "- Inputs:\n",
    "    - `memory`: List. Default: Empty List. The list containing the memory items\n",
    "    - `top_k`: Int. Default: 5. The number of memory list items to retrieve\n",
    "    - `mapper`: Function. Maps the memory item to another form for comparison by ranker or LLM. Default: `lambda x: x`\n",
    "        - Example mapping: `lambda x: x.fn_description` (If x is a Class and the string you want to compare for similarity is the fn_description attribute of that class)\n",
    "    - `approach`: str. Either `retrieve_by_ranker` or `retrieve_by_llm` to retrieve memory items.\n",
    "        - Ranker is faster and cheaper as it compares via embeddings, but are inferior to LLM-based methods for contextual information\n",
    "    - `retrieve_fn`: Default: None. Takes in task and outputs top_k similar memories in a list\n",
    "    - `ranker`: `Ranker`. The Ranker which defines a similarity score between a query and a key. Default: OpenAI `text-embedding-3-small` model. \n",
    "        - Can be replaced with a function which returns similarity score from 0 to 1 when given a query and key\n",
    "        \n",
    "## In-built Memory in AgentJo: Function Memory\n",
    "- Default: `memory_bank = {'Function': Memory(top_k = 5, mapper = lambda x: x.fn_description, approach = 'retrieve_by_ranker'), llm = self.llm}`\n",
    "- Does RAG over Task -> Function mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04abb86-6f74-4af5-8b72-b26a6029eb9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use Case 1: Filtering Functions by Task\n",
    "- AgentJo chooses `top k` (default k: 5) functions according to similarity to subtask\n",
    "- In addition to `top k` functions, we will also give agent all the compulsory functions\n",
    "    - `is_compulsory` variable of Function set to `True` means that we will always have it as one of the functions for planning and bypass Function RAG\n",
    "    \n",
    "## Example Use Case\n",
    "- Helps to reduce number of functions present in LLM context for more accurate generation\n",
    "```python\n",
    "output = my_agent.run('Calculate 2**10 * (5 + 1) / 10')\n",
    "```\n",
    "`Original Function List: add_numbers, subtract_numbers, add_three_numbers, multiply_numbers, divide_numbers, power_of, GCD_of_two_numbers, modulo_of_numbers, absolute_difference, generate_poem_with_numbers, List_related_words, generate_quote`\n",
    "\n",
    "`Filtered Function Names: add_three_numbers, multiply_numbers, divide_numbers, power_of, modulo_of_numbers`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72b4bd9f-b3b8-40e5-baec-3f41c616132a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import math\n",
    "def sum_numbers(num_list: List[float]) -> float:\n",
    "    '''Adds all numbers in num_list'''\n",
    "    return sum(x for x in num_list)\n",
    "\n",
    "def subtract_numbers(num1: float, num2: float) -> float:\n",
    "    '''Subtracts num1 from num2'''\n",
    "    return num1 - num2\n",
    "\n",
    "def multiply_numbers(num1: float, num2: float) -> float:\n",
    "    '''Multiplies num1 by num2'''\n",
    "    return num1 * num2\n",
    "\n",
    "def divide_numbers(num1: float, num2: float) -> float:\n",
    "    '''Divides num1 by num2'''\n",
    "    if num2 == 0:\n",
    "        return -1\n",
    "    return num1/num2\n",
    "\n",
    "def power_operation(num1: float, num2: float) -> float:\n",
    "    '''Returns num1 to the power of num2 (num1**num2)'''\n",
    "    return math.pow(num1, num2)\n",
    "\n",
    "def greatest_common_divisor(num1: int, num2: int) -> int:\n",
    "    '''Returns greatest common divisor of num1 and num2'''\n",
    "    return math.gcd(num1, num2)\n",
    "\n",
    "def modulo(num1: int, num2: int) -> int:\n",
    "    '''Returns modulo of num1 over num2'''\n",
    "    return num1%num2\n",
    "\n",
    "def absolute_difference(num1: int, num2: int) -> int:\n",
    "    '''Returns absolute difference between num1 and num2'''\n",
    "    return math.abs(num1-num2)\n",
    "\n",
    "# Put this to make sum_numbers always appear for any task and bypass Function RAG\n",
    "sum_numbers = Function(external_fn = sum_numbers, is_compulsory = True)\n",
    "\n",
    "# This is for Internal Functions\n",
    "generate_poem_with_numbers = Function(\"Generates a poem containing <num1: float> and <num2: float>\", output_format = {\"Poem\": \"Poem\"}, fn_name = 'generate_poem_with_numbers', llm = llm)\n",
    "list_related_words = Function(\"Lists out <num: int> words related to <word: str>\", output_format = {\"List of words\": \"List of words, type: list\"}, fn_name = 'list_related_words', llm = llm)\n",
    "generate_quote = Function(\"Generates a quote about <topic: str>\", output_format = {\"Quote\": \"Quote\"}, fn_name = 'generate_quote', llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aa5ac81-b850-44ca-a812-44fbe43d9943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_agent = Agent('Generalist Agent', \n",
    "'''Does everything''',\n",
    "                default_to_llm = False,\n",
    "                llm = llm).assign_functions([sum_numbers, subtract_numbers, multiply_numbers, \n",
    "            divide_numbers, power_operation, greatest_common_divisor, modulo, absolute_difference, \n",
    "            generate_poem_with_numbers, list_related_words, generate_quote])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1a95964-28c6-457b-bda2-49f750013d9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: end_task\n",
      "Description: Passes the final output to the user\n",
      "Input: []\n",
      "Output: {}\n",
      "\n",
      "Name: sum_numbers\n",
      "Description: Adds all numbers in <num_list: list[float]>\n",
      "Input: ['num_list']\n",
      "Output: {'output_1': 'float'}\n",
      "\n",
      "Name: subtract_numbers\n",
      "Description: Subtracts <num1: float> from <num2: float>\n",
      "Input: ['num1', 'num2']\n",
      "Output: {'output_1': 'float'}\n",
      "\n",
      "Name: multiply_numbers\n",
      "Description: Multiplies <num1: float> by <num2: float>\n",
      "Input: ['num1', 'num2']\n",
      "Output: {'output_1': 'float'}\n",
      "\n",
      "Name: divide_numbers\n",
      "Description: Divides <num1: float> by <num2: float>\n",
      "Input: ['num1', 'num2']\n",
      "Output: {'output_1': 'float'}\n",
      "\n",
      "Name: power_operation\n",
      "Description: Returns <num1: float> to the power of <num2: float> (<num1: float>**<num2: float>)\n",
      "Input: ['num1', 'num2']\n",
      "Output: {'output_1': 'float'}\n",
      "\n",
      "Name: greatest_common_divisor\n",
      "Description: Returns greatest common divisor of <num1: int> and <num2: int>\n",
      "Input: ['num1', 'num2']\n",
      "Output: {'output_1': 'int'}\n",
      "\n",
      "Name: modulo\n",
      "Description: Returns modulo of <num1: int> over <num2: int>\n",
      "Input: ['num1', 'num2']\n",
      "Output: {'output_1': 'int'}\n",
      "\n",
      "Name: absolute_difference\n",
      "Description: Returns absolute difference between <num1: int> and <num2: int>\n",
      "Input: ['num1', 'num2']\n",
      "Output: {'output_1': 'int'}\n",
      "\n",
      "Name: generate_poem_with_numbers\n",
      "Description: Generates a poem containing <num1: float> and <num2: float>\n",
      "Input: ['num1', 'num2']\n",
      "Output: {'Poem': 'Poem'}\n",
      "\n",
      "Name: list_related_words\n",
      "Description: Lists out <num: int> words related to <word: str>\n",
      "Input: ['num', 'word']\n",
      "Output: {'List of words': 'List of words, type: list'}\n",
      "\n",
      "Name: generate_quote\n",
      "Description: Generates a quote about <topic: str>\n",
      "Input: ['topic']\n",
      "Output: {'Quote': 'Quote'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see the auto-generated names of your functions :)\n",
    "my_agent.print_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90dbed97-b966-4dff-b3b6-7beceee67e76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure your top_k for function filtering here, default is 5\n",
    "my_agent.memory_bank['Function'].top_k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87357751-5f53-4e4b-adad-0487687d9ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modulo',\n",
       " 'subtract_numbers',\n",
       " 'multiply_numbers',\n",
       " 'divide_numbers',\n",
       " 'absolute_difference']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualise how the Functions are chosen based on task - here you see subtract_numbers appearing at the front\n",
    "# this does not include the compulsory functions\n",
    "[f.fn_name for f in my_agent.memory_bank['Function'].retrieve('Evaluate 3 - 1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ac84079-bb50-4fc8-83ac-167685a1b4a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Function Names: end_task, sum_numbers, subtract_numbers, multiply_numbers, divide_numbers, power_operation, modulo\n",
      "\u001b[1m\u001b[30mObservation: The task is to evaluate the expression 2 + 3. No subtasks have been completed yet.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mThoughts: To complete the assigned task, I need to perform the addition of the numbers 2 and 3.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[34mSubtask identified: Use the equipped function to add the numbers 2 and 3.\n",
      "\u001b[0m\n",
      "Calling function sum_numbers with parameters {'num_list': [2.0, 3.0]}\n",
      "> {'output_1': 5.0}\n",
      "\n",
      "Filtered Function Names: end_task, sum_numbers, subtract_numbers, multiply_numbers, divide_numbers, power_operation, modulo\n",
      "\u001b[1m\u001b[30mObservation: The sum of 2 and 3 has been calculated, resulting in 5.0.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mThoughts: Since the Assigned Task is to evaluate 2 + 3 and the result has already been computed, the next step is to finalize the task and present the output to the user.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[34mSubtask identified: End Task\u001b[0m\n",
      "Task completed successfully!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'output_1': 5.0}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_agent.reset()\n",
    "my_agent.run('Evaluate 2+3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed842f9c-84b1-4176-9f70-e97f35e65639",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Function Names: end_task, sum_numbers, subtract_numbers, multiply_numbers, divide_numbers, power_operation, modulo\n",
      "\u001b[1m\u001b[30mObservation: The Assigned Task requires evaluating the expression 2**10 * (3+5) / 10. \n",
      "No subtasks have been completed yet.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mThoughts: To complete the Assigned Task, I need to break down the expression into manageable parts. \n",
      "First, I can calculate the exponentiation (2**10), then evaluate the addition (3+5), \n",
      "and finally perform the multiplication and division in sequence.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[34mSubtask identified: Calculate the value of 2**10 and store the result for further operations.\n",
      "\u001b[0m\n",
      "Calling function power_operation with parameters {'num1': 2.0, 'num2': 10.0}\n",
      "> {'output_1': 1024.0}\n",
      "\n",
      "Filtered Function Names: end_task, sum_numbers, subtract_numbers, multiply_numbers, divide_numbers, power_operation, modulo\n",
      "\u001b[1m\u001b[30mObservation: The power operation has been completed, resulting in 1024.0 from 2 raised to the power of 10.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mThoughts: To complete the remainder of the assigned task, I need to evaluate the expression (3 + 5) and then multiply the result by 1024.0. After that, I will divide the product by 10 to get the final result.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[34mSubtask identified: Calculate (3 + 5) and then multiply the result by 1024.0.\n",
      "\u001b[0m\n",
      "Calling function sum_numbers with parameters {'num_list': [3.0, 5.0]}\n",
      "> {'output_1': 8.0}\n",
      "\n",
      "Filtered Function Names: end_task, sum_numbers, subtract_numbers, multiply_numbers, divide_numbers, power_operation, modulo\n",
      "\u001b[1m\u001b[30mObservation: The power operation has been completed, resulting in 1024.0. \n",
      "The sum of 3 and 5 has also been calculated, yielding 8.0. \n",
      "The next step is to multiply 1024.0 by 8.0 and then divide the result by 10.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mThoughts: To complete the remainder of the Assigned Task, I need to multiply the result of the power operation (1024.0) by the result of the sum (8.0), \n",
      "and then divide that product by 10 to finalize the evaluation.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[34mSubtask identified: Multiply 1024.0 by 8.0 and then divide the result by 10.\n",
      "\u001b[0m\n",
      "Calling function multiply_numbers with parameters {'num1': 1024.0, 'num2': 8.0}\n",
      "> {'output_1': 8192.0}\n",
      "\n",
      "Filtered Function Names: end_task, sum_numbers, subtract_numbers, multiply_numbers, divide_numbers, power_operation, modulo\n",
      "\u001b[1m\u001b[30mObservation: The subtasks completed include calculating 2**10 which resulted in 1024.0, summing 3 and 5 to get 8.0, and multiplying these results to get 8192.0. The final division by 10 is still pending.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mThoughts: To complete the remainder of the assigned task, I need to divide the result of 8192.0 by 10 to get the final output.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[34mSubtask identified: Perform the division of 8192.0 by 10.\n",
      "\u001b[0m\n",
      "Calling function divide_numbers with parameters {'num1': 8192.0, 'num2': 10.0}\n",
      "> {'output_1': 819.2}\n",
      "\n",
      "Filtered Function Names: end_task, sum_numbers, subtract_numbers, multiply_numbers, divide_numbers, power_operation, modulo\n",
      "\u001b[1m\u001b[30mObservation: The calculation for the Assigned Task has been completed successfully. The steps involved evaluating 2**10, summing 3 and 5, multiplying the results, and finally dividing by 10. The final output is 819.2.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mThoughts: Since the Assigned Task has been fully evaluated and the final result has been obtained, there is no further action required. The task is complete.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[34mSubtask identified: End Task\u001b[0m\n",
      "Task completed successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_agent.reset()\n",
    "output = my_agent.run('Evaluate 2**10 * (3+5) / 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a70c62c6-5325-47c2-a696-8b582809fcf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Observation': 'The Assigned Task requires evaluating the expression 2**10 * (3+5) / 10. \\nNo subtasks have been completed yet.\\n',\n",
       "  'Thoughts': 'To complete the Assigned Task, I need to break down the expression into manageable parts. \\nFirst, I can calculate the exponentiation (2**10), then evaluate the addition (3+5), \\nand finally perform the multiplication and division in sequence.\\n',\n",
       "  'Current Subtask': 'Calculate the value of 2**10 and store the result for further operations.\\n',\n",
       "  'Equipped Function Name': 'power_operation',\n",
       "  'Equipped Function Inputs': {'num1': 2.0, 'num2': 10.0}},\n",
       " {'Observation': 'The power operation has been completed, resulting in 1024.0 from 2 raised to the power of 10.\\n',\n",
       "  'Thoughts': 'To complete the remainder of the assigned task, I need to evaluate the expression (3 + 5) and then multiply the result by 1024.0. After that, I will divide the product by 10 to get the final result.\\n',\n",
       "  'Current Subtask': 'Calculate (3 + 5) and then multiply the result by 1024.0.\\n',\n",
       "  'Equipped Function Name': 'sum_numbers',\n",
       "  'Equipped Function Inputs': {'num_list': [3.0, 5.0]}},\n",
       " {'Observation': 'The power operation has been completed, resulting in 1024.0. \\nThe sum of 3 and 5 has also been calculated, yielding 8.0. \\nThe next step is to multiply 1024.0 by 8.0 and then divide the result by 10.\\n',\n",
       "  'Thoughts': 'To complete the remainder of the Assigned Task, I need to multiply the result of the power operation (1024.0) by the result of the sum (8.0), \\nand then divide that product by 10 to finalize the evaluation.\\n',\n",
       "  'Current Subtask': 'Multiply 1024.0 by 8.0 and then divide the result by 10.\\n',\n",
       "  'Equipped Function Name': 'multiply_numbers',\n",
       "  'Equipped Function Inputs': {'num1': 1024.0, 'num2': 8.0}},\n",
       " {'Observation': 'The subtasks completed include calculating 2**10 which resulted in 1024.0, summing 3 and 5 to get 8.0, and multiplying these results to get 8192.0. The final division by 10 is still pending.\\n',\n",
       "  'Thoughts': 'To complete the remainder of the assigned task, I need to divide the result of 8192.0 by 10 to get the final output.\\n',\n",
       "  'Current Subtask': 'Perform the division of 8192.0 by 10.\\n',\n",
       "  'Equipped Function Name': 'divide_numbers',\n",
       "  'Equipped Function Inputs': {'num1': 8192.0, 'num2': 10.0}},\n",
       " {'Observation': 'The calculation for the Assigned Task has been completed successfully. The steps involved evaluating 2**10, summing 3 and 5, multiplying the results, and finally dividing by 10. The final output is 819.2.\\n',\n",
       "  'Thoughts': 'Since the Assigned Task has been fully evaluated and the final result has been obtained, there is no further action required. The task is complete.\\n',\n",
       "  'Current Subtask': 'End the task by passing the final output to the user.\\n',\n",
       "  'Equipped Function Name': 'end_task',\n",
       "  'Equipped Function Inputs': {}}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_agent.thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d0a41b3-b7ee-4763-a73f-4fbd9daaf181",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To evaluate the expression 2**10 * (3+5) / 10, we can break it down using the subtasks that have already been completed.\\n\\n1. **Calculate 2**10**: \\n   - This is equivalent to `power_operation(num1=2.0, num2=10.0)`, which has been completed with an output of **1024.0**.\\n\\n2. **Calculate (3+5)**: \\n   - This is equivalent to `sum_numbers(num_list=[3.0, 5.0])`, which has been completed with an output of **8.0**.\\n\\n3. **Multiply the results of the first two steps**: \\n   - We need to calculate **1024.0 * 8.0**. This is equivalent to `multiply_numbers(num1=1024.0, num2=8.0)`, which has been completed with an output of **8192.0**.\\n\\n4. **Divide the result by 10**: \\n   - Finally, we need to calculate **8192.0 / 10.0**. This is equivalent to `divide_numbers(num1=8192.0, num2=10.0)`, which has been completed with an output of **819.2**.\\n\\nTherefore, the final result of the expression 2**10 * (3+5) / 10 is **819.2**.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_agent.reply_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ba75a65-7590-4552-a461-ec717d58a1f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Name: Generalist Agent\n",
      "Agent Description: Does everything\n",
      "Available Functions: ['end_task', 'sum_numbers', 'subtract_numbers', 'multiply_numbers', 'divide_numbers', 'power_operation', 'greatest_common_divisor', 'modulo', 'absolute_difference', 'generate_poem_with_numbers', 'list_related_words', 'generate_quote']\n",
      "Shared Variables: ['agent']\n",
      "\u001b[1m\u001b[32mTask: Evaluate 2**10 * (3+5) / 10\u001b[0m\n",
      "\u001b[1m\u001b[30mSubtasks Completed:\u001b[0m\n",
      "\u001b[1m\u001b[34mSubtask: power_operation(num1=2.0, num2=10.0)\u001b[0m\n",
      "{'output_1': 1024.0}\n",
      "\n",
      "\u001b[1m\u001b[34mSubtask: sum_numbers(num_list=[3.0, 5.0])\u001b[0m\n",
      "{'output_1': 8.0}\n",
      "\n",
      "\u001b[1m\u001b[34mSubtask: multiply_numbers(num1=1024.0, num2=8.0)\u001b[0m\n",
      "{'output_1': 8192.0}\n",
      "\n",
      "\u001b[1m\u001b[34mSubtask: divide_numbers(num1=8192.0, num2=10.0)\u001b[0m\n",
      "{'output_1': 819.2}\n",
      "\n",
      "\u001b[1m\u001b[34mSubtask: Replying User Query: Evaluate 2**10 * (3+5) / 10\u001b[0m\n",
      "To evaluate the expression 2**10 * (3+5) / 10, we can break it down using the subtasks that have already been completed.\n",
      "\n",
      "1. **Calculate 2**10**: \n",
      "   - This is equivalent to `power_operation(num1=2.0, num2=10.0)`, which has been completed with an output of **1024.0**.\n",
      "\n",
      "2. **Calculate (3+5)**: \n",
      "   - This is equivalent to `sum_numbers(num_list=[3.0, 5.0])`, which has been completed with an output of **8.0**.\n",
      "\n",
      "3. **Multiply the results of the first two steps**: \n",
      "   - We need to calculate **1024.0 * 8.0**. This is equivalent to `multiply_numbers(num1=1024.0, num2=8.0)`, which has been completed with an output of **8192.0**.\n",
      "\n",
      "4. **Divide the result by 10**: \n",
      "   - Finally, we need to calculate **8192.0 / 10.0**. This is equivalent to `divide_numbers(num1=8192.0, num2=10.0)`, which has been completed with an output of **819.2**.\n",
      "\n",
      "Therefore, the final result of the expression 2**10 * (3+5) / 10 is **819.2**.\n",
      "\n",
      "\n",
      "Is Task Completed: True\n"
     ]
    }
   ],
   "source": [
    "my_agent.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6260e2ee-5eae-44fb-8b70-76da88e16822",
   "metadata": {},
   "source": [
    "## Use Case 2: Adding more context based on task\n",
    "- You can add task-dependent context to the memory_bank so that `top k` will be added to prompt based on task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ace029-fde1-42f7-b2e2-cb34431ad9e3",
   "metadata": {},
   "source": [
    "### Using `memory_bank` for more context\n",
    "- Here, we have a mapping of nonsense words to numbers\n",
    "- Based on the subtask, we will augment the system prompt with relevant mappings to aid planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79cf809e-f14f-4c56-b070-87289a22df25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first append more context to the agent\n",
    "my_agent = Agent('Poem Creator', 'Create a poem according to a name', llm = llm)\n",
    "my_agent.memory_bank['Task Instructions'] = Memory(['For John, generate a poem about the sun', \n",
    "'For Mary, generate a poem about the wind', \n",
    "'For Peter, generate a poem about the sea'], # some task-specific instructions\n",
    "            top_k = 1,  # choose top 1\n",
    "            mapper = lambda x: x) # we compare with the task using only the first word, e.g. Azo, Boneti, Andkh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60aa15b8-9bf5-4339-a264-11ea93157164",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[30mObservation: A poem about John has not yet been created, but the task is to generate a poem about the sun for him.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mThoughts: To complete the remainder of the Assigned Task, I need to create a poem that captures the essence of the sun, focusing on its beauty, warmth, and significance.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[34mSubtask identified: Generate the first stanza of a poem about the sun for John.\n",
      "\u001b[0m\n",
      "Getting LLM to perform the following task: Generate the first stanza of a poem about the sun for John.\n",
      "\n",
      "> In the morning's gentle embrace, the sun does rise,  \n",
      "A golden orb that paints the skies,  \n",
      "With rays of warmth, it greets the day,  \n",
      "Chasing the shadows, lighting the way.  \n",
      "\n",
      "\n",
      "\u001b[1m\u001b[30mObservation: A stanza about the sun for John has been successfully generated, capturing its beauty and warmth.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mThoughts: To complete the remainder of the poem for John, I need to generate additional stanzas that continue the theme of the sun, perhaps exploring its impact on nature or emotions.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[34mSubtask identified: Generate the second stanza of a poem about the sun for John.\n",
      "\u001b[0m\n",
      "Getting LLM to perform the following task: Generate the second stanza of a poem about the sun for John.\n",
      "\n",
      "> As noon ascends, the sun stands tall,  \n",
      "A beacon bright, it casts its call,  \n",
      "With laughter in its golden light,  \n",
      "It dances on the waves, a joyful sight.  \n",
      "\n",
      "\n",
      "\u001b[1m\u001b[30mObservation: Two stanzas of a poem about the sun for John have been completed, capturing the morning and noon aspects of the sun's journey.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mThoughts: To complete the poem, I need to generate the final stanza that reflects the sunset and the transition into night, bringing closure to the poem for John.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[34mSubtask identified: Generate the third stanza of a poem about the sun for John, focusing on the sunset and its beauty.\n",
      "\u001b[0m\n",
      "Getting LLM to perform the following task: Generate the third stanza of a poem about the sun for John, focusing on the sunset and its beauty.\n",
      "\n",
      "> As evening falls, the sun dips low,  \n",
      "A canvas painted in hues aglow,  \n",
      "With strokes of orange, pink, and red,  \n",
      "It whispers softly, \"Day is dead.\"  \n",
      "\n",
      "\n",
      "\u001b[1m\u001b[30mObservation: Three stanzas of a poem about the sun for John have been completed, covering the morning, noon, and sunset.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mThoughts: To complete the remainder of the assigned task, I need to compile the stanzas into a cohesive poem and finalize it for John.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[34mSubtask identified: End Task\u001b[0m\n",
      "Task completed successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If name is present in memory, can use this information (e.g. John -> sun)\n",
    "my_agent.reset()\n",
    "output = my_agent.run('John')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb0e121c-d22f-42ec-a1c7-88fbb7af44f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[30mObservation: No subtasks have been completed for the task of creating a poem about Isaac Newton.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mThoughts: To complete the remainder of the task, I need to generate a poem that reflects the life and contributions of Isaac Newton, focusing on his discoveries in physics and mathematics.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[34mSubtask identified: Generate a poem about Isaac Newton using the equipped function.\n",
      "\u001b[0m\n",
      "Getting LLM to perform the following task: Generate a poem about Isaac Newton using the equipped function.\n",
      "\n",
      "> In the realm of thought, where shadows play,\n",
      "Isaac Newton, a mind that lit the way.\n",
      "Beneath the apple tree, a moment profound,\n",
      "The laws of motion and gravity he found.\n",
      "\n",
      "With a gaze to the heavens, he pondered the night,\n",
      "The orbs in their dance, a celestial sight.\n",
      "From calculus' birth to the prism's bright hue,\n",
      "He unveiled the secrets, the universe anew.\n",
      "\n",
      "A force that pulls, a force that guides,\n",
      "In every falling leaf, his wisdom abides.\n",
      "The world spins in harmony, a cosmic ballet,\n",
      "Thanks to Newton's brilliance, we understand today.\n",
      "\n",
      "So let us remember, with gratitude and cheer,\n",
      "The legacy of Isaac, whose vision is clear.\n",
      "A poet of science, with verses so grand,\n",
      "He penned the laws of nature, a guiding hand.\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[30mObservation: A poem about Isaac Newton has been successfully generated, capturing his contributions to science and his legacy.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mThoughts: To complete the remainder of the Assigned Task, I need to ensure that the final output is delivered to the user. This involves using the equipped function that passes the final output.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[34mSubtask identified: End Task\u001b[0m\n",
      "Task completed successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If the name is not in the memory, it will be interpreted according to the actual input\n",
    "my_agent.reset()\n",
    "output = my_agent.run('Isaac Newton')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb509e2",
   "metadata": {},
   "source": [
    "## Use Case 3: Add memory directly using pdf, docx, csv, xls files\n",
    "Adding memory elements one by one can be cumbersome, AgentJo memory can take filepath as input and it will split the text content inside the file path either using default splitter or user provided splitter.\n",
    "\n",
    "- We currently support `pdf`, `docx`, `csv`, `xls` files\n",
    "- You can also input own `text_splitter` function which takes in text and returns a list of splitted text by that function. Default: LangChain's RecursiveCharacterTextSplitter\n",
    "- We recommend the async method for faster accessing of memory\n",
    "\n",
    "Example:\n",
    "\n",
    "`memory = Memory(top_k = 5)`\n",
    "\n",
    "`memory.add_file(file_path)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "829dbc54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## Run the sync version of add_file (Takes very long)\n",
    "\n",
    "# import time\n",
    "\n",
    "# start_time = time.time()\n",
    "# memory = Memory(top_k = 1)\n",
    "# memory.add_file(filepath=\"./react.pdf\")\n",
    "# memory.retrieve('What is react')\n",
    "\n",
    "# end_time = time.time()\n",
    "# print(\"Time taken to run the code:\", end_time - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d697ee92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run the code: 2.986531972885132 seconds\n"
     ]
    }
   ],
   "source": [
    "# Async version of memory (Faster)\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "async_memory = AsyncMemory(top_k = 3)\n",
    "async_memory.add_file(filepath=\"./react.pdf\")\n",
    "await async_memory.retrieve('What is react')\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Time taken to run the code:\", end_time - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d457badb-ffba-42c8-8e38-c7c1876243c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def llm_async(system_prompt: str, user_prompt: str):\n",
    "    ''' Here, we use OpenAI for illustration, you can change it to your own LLM '''\n",
    "    # ensure your LLM imports are all within this function\n",
    "    from openai import AsyncOpenAI\n",
    "    \n",
    "    # define your own LLM here\n",
    "    client = AsyncOpenAI()\n",
    "    response = await client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        temperature = 0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ed211d2-fedf-4172-a7c3-5f3c3b3d3513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Equip this memory to the Agent and use it to answer a question\n",
    "agent = AsyncAgent('Content Answerer', 'Replies to questions factually based on given memory', llm = llm_async)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9253c639-df16-4479-8ad6-601b4a091563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.memory_bank['Document'] = async_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b435042-51d7-4c0a-9398-28da4dd327d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['erage success rate of 71%, signiﬁcantly outperforming\\nthe best Act (45%) and BUTLER (37%) trials. In fact, even the worse ReAct trial (48%) beats\\nthe best trial of both methods. Moreover, the advantage of ReAct overAct is consistent across\\nsix controlled trials, with relative performance gain ranging from 33% to 90% and averaging 62%.\\nQualitatively, we saw that, without any thoughts at all, Act fails to correctly decompose goals\\ninto smaller subgoals, or loses track of the current state of the environment. Example trajectories\\ncomparing ReAct andAct can be found in Appendix D.2.1 and Appendix D.2.2.\\nOn Webshop, one-shot Act prompting already performs on par with IL and IL+RL methods. With\\nadditional sparse reasoning, ReAct achieves signiﬁcantly better performance, with an absolute 10%\\nimprovement over the previous best success rate. By checking examples, we ﬁnd that ReAct is more\\nlikely to identify instruction-relevant products and options by reasoning to bridge the gap between\\nnoisy',\n",
       " 'ign for each task in Sections 3 and 4. B) General and ﬂexible : Due to the ﬂexible thought\\nspace and thought-action occurrence format, ReAct works for diverse tasks with distinct action\\nspaces and reasoning needs, including but not limited to QA, fact veriﬁcation, text game, and web\\nnavigation. C) Performant and robust :ReAct shows strong generalization to new task instances\\nwhile learning solely from one to six in-context examples, consistently outperforming baselines with\\nonly reasoning or acting across different domains. We also show in Section 3 additional beneﬁts\\nwhen ﬁnetuning is enabled, and in Section 4 how ReAct performance is robust to prompt selections.\\nD) Human aligned and controllable :ReAct promises an interpretable sequential decision making\\nand reasoning process where humans can easily inspect reasoning and factual correctness. Moreover,\\nhumans can also control or correct the agent behavior on the go by thought editing, as shown in\\nFigure 5 in Section 4.\\n3 K NOWLEDGE',\n",
       " 'ng with language\\nmodels for solving diverse language reasoning and decision making tasks (Figure 1). ReAct\\nprompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an\\ninterleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and\\nadjust high-level plans for acting (reason to act), while also interact with the external environments\\n(e.g. Wikipedia) to incorporate additional information into reasoning (act to reason).\\n2\\nPublished as a conference paper at ICLR 2023\\nWe conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks:\\nquestion answering (HotPotQA, Yang et al., 2018), fact veriﬁcation (Fever, Thorne et al., 2018),\\ntext-based game (ALFWorld, Shridhar et al., 2020b), and webpage navigation (WebShop, Yao\\net al., 2022). For HotPotQA and Fever, with access to a Wikipedia API that the model can interact\\nwith, ReAct outperforms vanilla action generation models while being competitive']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await async_memory.retrieve('What is ReAct?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5433055b-ac0c-416e-b2f8-20f9a76ce0c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[30mObservation: ReAct is a method that integrates reasoning and action in a flexible manner, allowing for dynamic decision-making across various tasks. It has shown a significant performance advantage over traditional methods like Act, particularly in tasks such as question answering and web navigation. The empirical evaluations indicate that ReAct can effectively generate reasoning traces and actions, leading to improved outcomes in diverse benchmarks.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mThoughts: To complete the remainder of the Assigned Task, I need to summarize the key features and benefits of ReAct, emphasizing its performance, flexibility, and human alignment. This will provide a comprehensive understanding of what ReAct is and how it functions in various contexts.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[34mSubtask identified: Summarize the key features and benefits of ReAct, focusing on its performance advantages, flexibility in handling diverse tasks, and its human-aligned decision-making process.\n",
      "\u001b[0m\n",
      "Getting LLM to perform the following task: Summarize the key features and benefits of ReAct, focusing on its performance advantages, flexibility in handling diverse tasks, and its human-aligned decision-making process.\n",
      "\n",
      "> ReAct is a versatile framework designed to enhance performance across a variety of tasks, showcasing several key features and benefits:\n",
      "\n",
      "- **Performance Advantages**: ReAct demonstrates a significant improvement in success rates compared to traditional methods. In benchmarks like HotpotQA and Fever, it effectively mitigates issues of hallucination and error propagation by utilizing a Wikipedia API for real-time information retrieval. This results in more accurate and interpretable task-solving trajectories, with ReAct achieving an average success rate of 71%, outperforming the best alternatives by a notable margin.\n",
      "\n",
      "- **Flexibility in Handling Diverse Tasks**: The framework is built to accommodate a wide range of action spaces and reasoning requirements. Its flexible thought-action occurrence format allows it to adapt to various tasks, including question answering, fact verification, text games, and web navigation. This adaptability is crucial for generalization to new task instances, as ReAct can learn effectively from just one to six in-context examples.\n",
      "\n",
      "- **Human-Aligned Decision-Making Process**: ReAct emphasizes interpretability and control in its decision-making process. It allows users to inspect the reasoning behind actions and correct the agent's behavior in real-time through thought editing. This human-aligned approach fosters trust and enhances the overall user experience, making it easier for users to engage with the system and ensure factual correctness.\n",
      "\n",
      "Overall, ReAct stands out for its robust performance, flexibility across diverse applications, and alignment with human reasoning, making it a powerful tool for language and decision-making tasks.\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[30mObservation: ReAct is a versatile framework designed to enhance performance across a variety of tasks, showcasing several key features and benefits, including significant performance advantages, flexibility in handling diverse tasks, and a human-aligned decision-making process. It achieves an average success rate of 71%, outperforming traditional methods and demonstrating strong generalization capabilities.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mThoughts: To complete the remainder of the Assigned Task, I need to provide a concise definition of ReAct, highlighting its core principles and applications. This will involve synthesizing the information gathered from the completed subtasks and knowledge references to ensure clarity and accuracy.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[34mSubtask identified: Provide a clear and concise definition of ReAct, including its purpose, key features, and applications in language reasoning and decision-making tasks.\n",
      "\u001b[0m\n",
      "Getting LLM to perform the following task: Provide a clear and concise definition of ReAct, including its purpose, key features, and applications in language reasoning and decision-making tasks.\n",
      "\n",
      "> ReAct is a cutting-edge framework designed to enhance language reasoning and decision-making tasks. Its primary purpose is to improve the performance and interpretability of models in various applications, including question answering, fact verification, text-based games, and web navigation.\n",
      "\n",
      "Key Features:\n",
      "- **Performance Improvement**: ReAct significantly boosts success rates by addressing common issues such as hallucination and error propagation. By leveraging a Wikipedia API for real-time information retrieval, it generates more accurate and interpretable task-solving trajectories.\n",
      "- **Flexibility**: The framework is adaptable to a wide range of action spaces and reasoning needs, allowing it to handle diverse tasks effectively. It can learn from as few as one to six in-context examples, making it robust in generalizing to new task instances.\n",
      "- **Human Alignment**: ReAct emphasizes an interpretable decision-making process, enabling users to inspect reasoning and correct agent behavior in real-time through thought editing. This human-aligned approach fosters trust and enhances user engagement.\n",
      "\n",
      "Applications:\n",
      "ReAct is utilized in various domains, including:\n",
      "- **Question Answering**: It excels in benchmarks like HotpotQA, achieving an average success rate of 71%.\n",
      "- **Fact Verification**: It effectively verifies facts by interacting with external knowledge sources.\n",
      "- **Interactive Decision Making**: In environments like ALFWorld and WebShop, ReAct outperforms traditional methods, showcasing its versatility and effectiveness.\n",
      "\n",
      "Overall, ReAct stands out for its robust performance, flexibility across diverse applications, and alignment with human reasoning, making it a powerful tool for language and decision-making tasks.\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[30mObservation: The key features and benefits of ReAct have been summarized, highlighting its performance advantages, flexibility in handling diverse tasks, and its human-aligned decision-making process. Additionally, a clear and concise definition of ReAct has been provided, detailing its purpose, key features, and applications in language reasoning and decision-making tasks.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mThoughts: To complete the remainder of the Assigned Task, I need to synthesize the information provided into a cohesive explanation that encapsulates the essence of ReAct, ensuring that it is both informative and accessible to the user.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[34mSubtask identified: Create a comprehensive overview of ReAct that integrates the summarized features, benefits, and applications into a single, coherent response suitable for the user.\n",
      "\u001b[0m\n",
      "Getting LLM to perform the following task: Create a comprehensive overview of ReAct that integrates the summarized features, benefits, and applications into a single, coherent response suitable for the user.\n",
      "\n",
      "> ReAct is an innovative framework designed to enhance language reasoning and decision-making tasks, showcasing a range of features and benefits that make it a powerful tool in various applications. \n",
      "\n",
      "**Key Features and Benefits**:\n",
      "- **Performance Improvement**: ReAct significantly boosts success rates in tasks such as question answering and fact verification. By utilizing a Wikipedia API for real-time information retrieval, it effectively mitigates issues of hallucination and error propagation, resulting in more accurate and interpretable task-solving trajectories. In benchmarks like HotpotQA, ReAct achieves an impressive average success rate of 71%, outperforming traditional methods by a notable margin.\n",
      "\n",
      "- **Flexibility in Handling Diverse Tasks**: The framework is built to accommodate a wide range of action spaces and reasoning requirements. Its flexible thought-action occurrence format allows it to adapt to various tasks, including text games and web navigation. This adaptability is crucial for generalization to new task instances, as ReAct can learn effectively from just one to six in-context examples, making it robust across different scenarios.\n",
      "\n",
      "- **Human-Aligned Decision-Making Process**: ReAct emphasizes interpretability and control in its decision-making process. Users can inspect the reasoning behind actions and correct the agent's behavior in real-time through thought editing. This human-aligned approach fosters trust and enhances the overall user experience, making it easier for users to engage with the system and ensure factual correctness.\n",
      "\n",
      "**Applications**:\n",
      "ReAct is utilized in various domains, including:\n",
      "- **Question Answering**: Excelling in benchmarks like HotpotQA, it achieves high success rates and provides reliable answers.\n",
      "- **Fact Verification**: Effectively verifies facts by interacting with external knowledge sources, ensuring accuracy in information dissemination.\n",
      "- **Interactive Decision Making**: In environments like ALFWorld and WebShop, ReAct outperforms traditional methods, showcasing its versatility and effectiveness in dynamic settings.\n",
      "\n",
      "Overall, ReAct stands out for its robust performance, flexibility across diverse applications, and alignment with human reasoning, making it an essential tool for enhancing language and decision-making tasks.\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[30mObservation: The key features and benefits of ReAct have been summarized, highlighting its performance advantages, flexibility in handling diverse tasks, and its human-aligned decision-making process. Additionally, a clear and concise definition of ReAct has been provided, detailing its purpose, key features, and applications in language reasoning and decision-making tasks. A comprehensive overview has also been created, integrating the summarized features, benefits, and applications into a coherent response.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mThoughts: To complete the remainder of the Assigned Task, I need to ensure that the definition and overview of ReAct are presented in a user-friendly manner. This may involve refining the language or formatting the information for clarity and ease of understanding.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[34mSubtask identified: Refine the definition and overview of ReAct to ensure clarity and user-friendliness, making it suitable for presentation to the user.\n",
      "\u001b[0m\n",
      "Getting LLM to perform the following task: Refine the definition and overview of ReAct to ensure clarity and user-friendliness, making it suitable for presentation to the user.\n",
      "\n",
      "> ReAct is an advanced framework designed to enhance language reasoning and decision-making tasks, making it a powerful tool across various applications. Its primary goal is to improve the performance and interpretability of models in tasks such as question answering, fact verification, text-based games, and web navigation.\n",
      "\n",
      "**Key Features and Benefits**:\n",
      "- **Performance Improvement**: ReAct significantly enhances success rates by addressing common challenges like hallucination and error propagation. By leveraging a Wikipedia API for real-time information retrieval, it generates more accurate and interpretable task-solving trajectories. For instance, in benchmarks like HotpotQA, ReAct achieves an impressive average success rate of 71%, outperforming traditional methods by a substantial margin.\n",
      "\n",
      "- **Flexibility in Handling Diverse Tasks**: The framework is designed to accommodate a wide range of action spaces and reasoning requirements. Its flexible thought-action occurrence format allows it to adapt to various tasks effectively, including text games and web navigation. This adaptability is crucial for generalization to new task instances, as ReAct can learn from as few as one to six in-context examples, making it robust across different scenarios.\n",
      "\n",
      "- **Human-Aligned Decision-Making Process**: ReAct emphasizes interpretability and control in its decision-making process. Users can inspect the reasoning behind actions and correct the agent's behavior in real-time through thought editing. This human-aligned approach fosters trust and enhances the overall user experience, making it easier for users to engage with the system and ensure factual correctness.\n",
      "\n",
      "**Applications**:\n",
      "ReAct is utilized in various domains, including:\n",
      "- **Question Answering**: It excels in benchmarks like HotpotQA, achieving high success rates and providing reliable answers.\n",
      "- **Fact Verification**: It effectively verifies facts by interacting with external knowledge sources, ensuring accuracy in information dissemination.\n",
      "- **Interactive Decision Making**: In environments like ALFWorld and WebShop, ReAct outperforms traditional methods, showcasing its versatility and effectiveness in dynamic settings.\n",
      "\n",
      "Overall, ReAct stands out for its robust performance, flexibility across diverse applications, and alignment with human reasoning, making it an essential tool for enhancing language and decision-making tasks.\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[30mObservation: The key features and benefits of ReAct have been summarized, highlighting its performance advantages, flexibility in handling diverse tasks, and its human-aligned decision-making process. Additionally, a clear and concise definition of ReAct has been provided, along with a comprehensive overview that integrates these elements into a coherent response suitable for the user.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mThoughts: To complete the remainder of the Assigned Task, I need to ensure that the definition and overview of ReAct are refined for clarity and user-friendliness, making it suitable for presentation to the user. This involves synthesizing the information already gathered and presenting it in a straightforward manner.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[34mSubtask identified: Refine the definition and overview of ReAct to ensure clarity and user-friendliness, making it suitable for presentation to the user.\n",
      "\u001b[0m\n",
      "Getting LLM to perform the following task: Refine the definition and overview of ReAct to ensure clarity and user-friendliness, making it suitable for presentation to the user.\n",
      "\n",
      "> ReAct is an advanced framework designed to enhance language reasoning and decision-making tasks, making it a powerful tool across various applications. Its primary goal is to improve the performance and interpretability of models in tasks such as question answering, fact verification, text-based games, and web navigation.\n",
      "\n",
      "**Key Features and Benefits**:\n",
      "- **Performance Improvement**: ReAct significantly enhances success rates by addressing common challenges like hallucination and error propagation. By leveraging a Wikipedia API for real-time information retrieval, it generates more accurate and interpretable task-solving trajectories. For instance, in benchmarks like HotpotQA, ReAct achieves an impressive average success rate of 71%, outperforming traditional methods by a substantial margin.\n",
      "\n",
      "- **Flexibility in Handling Diverse Tasks**: The framework is designed to accommodate a wide range of action spaces and reasoning requirements. Its flexible thought-action occurrence format allows it to adapt to various tasks effectively, including text games and web navigation. This adaptability is crucial for generalization to new task instances, as ReAct can learn from as few as one to six in-context examples, making it robust across different scenarios.\n",
      "\n",
      "- **Human-Aligned Decision-Making Process**: ReAct emphasizes interpretability and control in its decision-making process. Users can inspect the reasoning behind actions and correct the agent's behavior in real-time through thought editing. This human-aligned approach fosters trust and enhances the overall user experience, making it easier for users to engage with the system and ensure factual correctness.\n",
      "\n",
      "**Applications**:\n",
      "ReAct is utilized in various domains, including:\n",
      "- **Question Answering**: It excels in benchmarks like HotpotQA, achieving high success rates and providing reliable answers.\n",
      "- **Fact Verification**: It effectively verifies facts by interacting with external knowledge sources, ensuring accuracy in information dissemination.\n",
      "- **Interactive Decision Making**: In environments like ALFWorld and WebShop, ReAct outperforms traditional methods, showcasing its versatility and effectiveness in dynamic settings.\n",
      "\n",
      "Overall, ReAct stands out for its robust performance, flexibility across diverse applications, and alignment with human reasoning, making it an essential tool for enhancing language and decision-making tasks.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"ReAct is a versatile framework designed to enhance performance across a variety of tasks, showcasing several key features and benefits:\\n\\n- **Performance Advantages**: ReAct demonstrates a significant improvement in success rates compared to traditional methods. In benchmarks like HotpotQA and Fever, it effectively mitigates issues of hallucination and error propagation by utilizing a Wikipedia API for real-time information retrieval. This results in more accurate and interpretable task-solving trajectories, with ReAct achieving an average success rate of 71%, outperforming the best alternatives by a notable margin.\\n\\n- **Flexibility in Handling Diverse Tasks**: The framework is built to accommodate a wide range of action spaces and reasoning requirements. Its flexible thought-action occurrence format allows it to adapt to various tasks, including question answering, fact verification, text games, and web navigation. This adaptability is crucial for generalization to new task instances, as ReAct can learn effectively from just one to six in-context examples.\\n\\n- **Human-Aligned Decision-Making Process**: ReAct emphasizes interpretability and control in its decision-making process. It allows users to inspect the reasoning behind actions and correct the agent's behavior in real-time through thought editing. This human-aligned approach fosters trust and enhances the overall user experience, making it easier for users to engage with the system and ensure factual correctness.\\n\\nOverall, ReAct stands out for its robust performance, flexibility across diverse applications, and alignment with human reasoning, making it a powerful tool for language and decision-making tasks.\\n\",\n",
       " 'ReAct is a cutting-edge framework designed to enhance language reasoning and decision-making tasks. Its primary purpose is to improve the performance and interpretability of models in various applications, including question answering, fact verification, text-based games, and web navigation.\\n\\nKey Features:\\n- **Performance Improvement**: ReAct significantly boosts success rates by addressing common issues such as hallucination and error propagation. By leveraging a Wikipedia API for real-time information retrieval, it generates more accurate and interpretable task-solving trajectories.\\n- **Flexibility**: The framework is adaptable to a wide range of action spaces and reasoning needs, allowing it to handle diverse tasks effectively. It can learn from as few as one to six in-context examples, making it robust in generalizing to new task instances.\\n- **Human Alignment**: ReAct emphasizes an interpretable decision-making process, enabling users to inspect reasoning and correct agent behavior in real-time through thought editing. This human-aligned approach fosters trust and enhances user engagement.\\n\\nApplications:\\nReAct is utilized in various domains, including:\\n- **Question Answering**: It excels in benchmarks like HotpotQA, achieving an average success rate of 71%.\\n- **Fact Verification**: It effectively verifies facts by interacting with external knowledge sources.\\n- **Interactive Decision Making**: In environments like ALFWorld and WebShop, ReAct outperforms traditional methods, showcasing its versatility and effectiveness.\\n\\nOverall, ReAct stands out for its robust performance, flexibility across diverse applications, and alignment with human reasoning, making it a powerful tool for language and decision-making tasks.\\n',\n",
       " \"ReAct is an innovative framework designed to enhance language reasoning and decision-making tasks, showcasing a range of features and benefits that make it a powerful tool in various applications. \\n\\n**Key Features and Benefits**:\\n- **Performance Improvement**: ReAct significantly boosts success rates in tasks such as question answering and fact verification. By utilizing a Wikipedia API for real-time information retrieval, it effectively mitigates issues of hallucination and error propagation, resulting in more accurate and interpretable task-solving trajectories. In benchmarks like HotpotQA, ReAct achieves an impressive average success rate of 71%, outperforming traditional methods by a notable margin.\\n\\n- **Flexibility in Handling Diverse Tasks**: The framework is built to accommodate a wide range of action spaces and reasoning requirements. Its flexible thought-action occurrence format allows it to adapt to various tasks, including text games and web navigation. This adaptability is crucial for generalization to new task instances, as ReAct can learn effectively from just one to six in-context examples, making it robust across different scenarios.\\n\\n- **Human-Aligned Decision-Making Process**: ReAct emphasizes interpretability and control in its decision-making process. Users can inspect the reasoning behind actions and correct the agent's behavior in real-time through thought editing. This human-aligned approach fosters trust and enhances the overall user experience, making it easier for users to engage with the system and ensure factual correctness.\\n\\n**Applications**:\\nReAct is utilized in various domains, including:\\n- **Question Answering**: Excelling in benchmarks like HotpotQA, it achieves high success rates and provides reliable answers.\\n- **Fact Verification**: Effectively verifies facts by interacting with external knowledge sources, ensuring accuracy in information dissemination.\\n- **Interactive Decision Making**: In environments like ALFWorld and WebShop, ReAct outperforms traditional methods, showcasing its versatility and effectiveness in dynamic settings.\\n\\nOverall, ReAct stands out for its robust performance, flexibility across diverse applications, and alignment with human reasoning, making it an essential tool for enhancing language and decision-making tasks.\\n\",\n",
       " \"ReAct is an advanced framework designed to enhance language reasoning and decision-making tasks, making it a powerful tool across various applications. Its primary goal is to improve the performance and interpretability of models in tasks such as question answering, fact verification, text-based games, and web navigation.\\n\\n**Key Features and Benefits**:\\n- **Performance Improvement**: ReAct significantly enhances success rates by addressing common challenges like hallucination and error propagation. By leveraging a Wikipedia API for real-time information retrieval, it generates more accurate and interpretable task-solving trajectories. For instance, in benchmarks like HotpotQA, ReAct achieves an impressive average success rate of 71%, outperforming traditional methods by a substantial margin.\\n\\n- **Flexibility in Handling Diverse Tasks**: The framework is designed to accommodate a wide range of action spaces and reasoning requirements. Its flexible thought-action occurrence format allows it to adapt to various tasks effectively, including text games and web navigation. This adaptability is crucial for generalization to new task instances, as ReAct can learn from as few as one to six in-context examples, making it robust across different scenarios.\\n\\n- **Human-Aligned Decision-Making Process**: ReAct emphasizes interpretability and control in its decision-making process. Users can inspect the reasoning behind actions and correct the agent's behavior in real-time through thought editing. This human-aligned approach fosters trust and enhances the overall user experience, making it easier for users to engage with the system and ensure factual correctness.\\n\\n**Applications**:\\nReAct is utilized in various domains, including:\\n- **Question Answering**: It excels in benchmarks like HotpotQA, achieving high success rates and providing reliable answers.\\n- **Fact Verification**: It effectively verifies facts by interacting with external knowledge sources, ensuring accuracy in information dissemination.\\n- **Interactive Decision Making**: In environments like ALFWorld and WebShop, ReAct outperforms traditional methods, showcasing its versatility and effectiveness in dynamic settings.\\n\\nOverall, ReAct stands out for its robust performance, flexibility across diverse applications, and alignment with human reasoning, making it an essential tool for enhancing language and decision-making tasks.\\n\",\n",
       " \"ReAct is an advanced framework designed to enhance language reasoning and decision-making tasks, making it a powerful tool across various applications. Its primary goal is to improve the performance and interpretability of models in tasks such as question answering, fact verification, text-based games, and web navigation.\\n\\n**Key Features and Benefits**:\\n- **Performance Improvement**: ReAct significantly enhances success rates by addressing common challenges like hallucination and error propagation. By leveraging a Wikipedia API for real-time information retrieval, it generates more accurate and interpretable task-solving trajectories. For instance, in benchmarks like HotpotQA, ReAct achieves an impressive average success rate of 71%, outperforming traditional methods by a substantial margin.\\n\\n- **Flexibility in Handling Diverse Tasks**: The framework is designed to accommodate a wide range of action spaces and reasoning requirements. Its flexible thought-action occurrence format allows it to adapt to various tasks effectively, including text games and web navigation. This adaptability is crucial for generalization to new task instances, as ReAct can learn from as few as one to six in-context examples, making it robust across different scenarios.\\n\\n- **Human-Aligned Decision-Making Process**: ReAct emphasizes interpretability and control in its decision-making process. Users can inspect the reasoning behind actions and correct the agent's behavior in real-time through thought editing. This human-aligned approach fosters trust and enhances the overall user experience, making it easier for users to engage with the system and ensure factual correctness.\\n\\n**Applications**:\\nReAct is utilized in various domains, including:\\n- **Question Answering**: It excels in benchmarks like HotpotQA, achieving high success rates and providing reliable answers.\\n- **Fact Verification**: It effectively verifies facts by interacting with external knowledge sources, ensuring accuracy in information dissemination.\\n- **Interactive Decision Making**: In environments like ALFWorld and WebShop, ReAct outperforms traditional methods, showcasing its versatility and effectiveness in dynamic settings.\\n\\nOverall, ReAct stands out for its robust performance, flexibility across diverse applications, and alignment with human reasoning, making it an essential tool for enhancing language and decision-making tasks.\\n\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await agent.run('What is ReAct?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
