{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eecd920b-ea8c-4890-ad76-227d7fd52964",
   "metadata": {},
   "source": [
    "# Deep Research v1\n",
    "- Does a single call of arxiv api to return top 10 sources\n",
    "- Extracts out only those information relevant to user query\n",
    "- Based on extracted information, compile a markdown file that does in-line citation for sources\n",
    "\n",
    "Improvements:\n",
    "- Iterative search with agentic framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ba9d88e-7e41-43b1-9fd8-f8ab1f62d3f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from agentjo import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5289ed5-3c23-4bb9-9d1c-9f02aec6d13a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "352da821-5a8f-4296-81a0-ff6e97c6eb70",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def llm(system_prompt: str, user_prompt: str) -> str:\n",
    "    ''' Here, we use OpenAI for illustration, you can change it to your own LLM '''\n",
    "    # ensure your LLM imports are all within this function\n",
    "    from openai import AsyncOpenAI\n",
    "    \n",
    "    # define your own LLM here\n",
    "    client = AsyncOpenAI()\n",
    "    response = await client.chat.completions.create(\n",
    "        model='o3-mini',\n",
    "        # temperature = 0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13cff782-2ae2-4fc8-b9be-1eff3e110dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your search query for arXiv papers:  memory adaptive neuroscience\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kabir2024deepreinforcementlearningtimescale\n",
      "1. Introduction: Memory in this context is the mechanism by which agents encode, store, and retrieve temporal information. It is key to learning relationships between events (cause and effect) and is inspired by biological systems where timing is critical for survival and decision-making. \n",
      "2. Types of Memory: The text highlights different memory architectures used in deep reinforcement learning. Standard types include recurrent neural networks (RNNs) and Long Short-Term Memory networks (LSTMs), while the advanced, cognitively inspired version is the scale invariant memory (also known as CogRNN). The scale invariant memory uses a Laplace transform-based approach to generate a log-compressed representation of temporal history, resembling time cells found in mammalian brains. \n",
      "3. How Memory can be adaptive: Memory becomes adaptive through scale invariance. In these systems, when temporal relationships are rescaled, the memory representation shifts (translates) rather than distorts. This shift means that learning remains equally effective regardless of the absolute time scale, enabling agents to function robustly in environments with varying temporal dynamics without the need for hyperparameter readjustment. \n",
      "4. How Memory schema is created: The memory schema is constructed using a two-layer neural network. The first layer is a recurrent module, which computes a modified Laplace transform of the input signal via a differential equation (dF/dt = -sF + f(t)). This provides an exponentially decayed, scale-invariant representation of past inputs. The second layer, a dense layer with analytically determined weights, performs an approximate inverse Laplace transform to create a log-compressed timeline of the past. Parameters such as ∗τ (tau), which are log-spaced (e.g., ∗τmin = 1 and ∗τmax = 1000) and k (order of derivative), are crucial in defining the precise shape of the resulting sequentially activated time cells. \n",
      "5. Future focus areas on memory: Future research may focus on integrating scale invariant (power-law) temporal discounting with these memory systems, extending their application beyond interval timing and discrimination to more complex, real-world tasks, and enhancing adaptability to varied environmental scales. This could lead to AI systems that require minimal hyperparameter tuning and can quickly adapt to new temporal dynamics. \n",
      "6. Conclusion: The study demonstrates that by incorporating a scale invariant memory—constructed using principles like the Laplace and inverse Laplace transforms—deep reinforcement learning agents can achieve learning performance that is robust across different time scales. This approach bridges insights from neuroscience and cognitive science to enhance the adaptability and efficiency of artificial agents in processing temporal information.\n",
      "wakeling2004adaptivityperlearning\n",
      "1. Introduction: The paper presents memory as an adaptive process in biological neural networks. Memory here is not merely the storage of information but also the capacity to adapt and “unlearn” outdated patterns based on environmental changes. It emphasizes the importance of adjusting synaptic connections, making memory a dynamic and continuously evolving property. \n",
      "2. Types of Memory: The text describes different memory formation mechanisms through synaptic modifications. One type is based on long-term depression (LTD) where incorrect or less-used connections are weakened, creating a “forgetful” memory that can be easily updated. Another involves long-term potentiation (LTP) which strengthens connections through positive feedback. The paper also contrasts unbounded potentiation (where strengthening can lead to runaway effects) with bounded potentiation (where an upper limit is set to prevent excessive strengthening) and examines a selective punishment method that marks successful synapses as “good” and makes them easier to reactivate if needed. \n",
      "3. How Memory can be adaptive: The adaptive nature of memory is achieved by modifying synaptic strengths in response to errors and new associations. When a network faces a new input-output map, synapses responsible for errors are punished (weakened), while previously effective pathways may be maintained with less severe punishment (selective punishment) or enhanced via LTP. This dynamic allows the system to quickly reconfigure its connections and switch between different learned patterns, thereby making memory adaptive. \n",
      "4. How Memory schema is created: Memory schemas are formed through a competitive dynamic between active (used) and inactive (unused) synapses. Over time, consistent patterns of correct responses lead to certain synapses being marked as “good” through reduced punishment rates, while incorrect paths are suppressed using LTD. This process organizes the network’s connectivity into a schema that represents the learned relationships between inputs and outputs. \n",
      "5. Future focus areas on memory: Future investigations should explore mechanisms to balance LTP and LTD so as to prevent the divergence in synaptic strengths. This includes implementing global feedback mechanisms or alternative positive feedback methods (such as synaptic forgiveness) to avoid runaway potentiation. Research may also focus on tuning the selective punishment parameters to refine the network’s adaptive memory, ensuring rapid reconfiguration without sacrificing long-term stability. \n",
      "6. Conclusion: The study concludes that memory in adaptive neural networks is inherently dynamic. The interplay between synaptic depression and potentiation allows a network to learn and, crucially, to unlearn and relearn as conditions change. Proper management of these processes is vital to achieve both the stability and flexibility needed in biological learning and memory systems.\n",
      "hoppensteadt2020frequencyphasepotentialforcedstno\n",
      "1. Introduction: The paper describes memory as a physical attribute that arises from valleys in potential landscapes created by oscillator networks. In these networks, the dynamics of oscillators are governed by potential functions, where a valley (or basin of attraction) in the amplitude or phase potential represents a memory unit. The system can store information through such landscapes, and these stable valleys can be accessed via associative recall.\n",
      "\n",
      "2. Types of Memory: The analysis distinguishes between two kinds of memory in the system. (a) Structural Memory (or time domain memory) is inherent in the free (unforced) dynamics of the system and remains sustained even if the external forcing is removed. (b) Evoked Memory (or frequency domain memory) is exhibited only when the system is externally driven; it is represented by the phase potential, where the valleys corresponding to certain phase states are stable under forced conditions.\n",
      "\n",
      "3. How Memory can be adaptive: Memory in the system is adaptive because the external forcing dynamically reorganizes the potential landscape. The forcing scales and organizes oscillators into clusters that lock to particular frequencies. This means that the memory (i.e., the stable valleys in the phase potential) can shift or adapt based on changes in the forcing amplitude, frequency, or phase deviations. Such adaptability is illustrated by the formation of phase locking intervals, whose widths depend on the power in the forcing signal, allowing the memory to respond to changing environmental inputs.\n",
      "\n",
      "4. How Memory schema is created: The memory schema is created by deriving the amplitude potential V and the phase potential W from the oscillator’s differential equations. In these potentials, minima (or valleys) correspond to stable equilibrium states. For the amplitude, a single positive minimum often exists under certain parameter conditions (e.g., b > 0, a specific value of \u0016), while for phase deviations, the potential W forms wells that are dictated by the forcing frequencies and amplitudes. When the system is initialized within one of these valleys, it tends to converge toward that minimum, forming a reliable memory state that can later be recalled by associative methods.\n",
      "\n",
      "5. Future focus areas on memory: Future research may focus on further quantifying the structure of these potential wells by analyzing the depth and width of the valleys in the phase potential W as functions of forcing parameters. Investigations may also include exploring global feedback mechanisms that synchronize the oscillator population and studying how these memory states respond to random noise and chaotic perturbations. Additionally, extensions to continuum models (like spin wave interference and vortex solutions) might provide insights into more complex memory and computation schemes in neuromorphic systems.\n",
      "\n",
      "6. Conclusion: In summary, memory in the forced oscillator network is established through the creation of potential landscapes with distinct valleys that act as memory units. The two types of memory—structural and evoked—highlight different mechanisms by which the system stores and processes information. Structural memory is maintained intrinsically in the network, while evoked memory is dynamically created through external forcing. This understanding opens pathways to advanced applications in pattern recognition, associative recall, and neuromorphic engineering, underscoring the system’s potential in both computation and control.\n",
      "lakhman2012neuroevolutionresultsemergenceshortterm\n",
      "1. Introduction: In this context, memory refers to the ability of an agent to store and recall recent past states or actions (short-term memory) that are crucial for making decisions in environments with multiple goals. Memory allows the agent to base its current behavior on its previous actions, enabling alternative behaviors even when faced with the same environmental state.\n",
      "\n",
      "2. Types of Memory: The study focuses on short-term memory (STM) within recurrent neural networks. Two principal types of mechanisms are identified: (a) A mechanism based on the integration of sensory inputs with ongoing internal neural activity via recurrent connections (reverberation), leading to specialized groups of neurons that support alternative actions. (b) A mechanism based on slow neurodynamical oscillatory processes, where the neuron’s output dynamics enable the coding of previous behavioral choices. Quantitatively, the depth of memory was found to vary with a lower bound of storing at least 4 past states, and in some cases, memory effects covering up to 30 previous states were observed.\n",
      "\n",
      "3. How Memory can be Adaptive: Memory enables adaptive behavior by allowing agents to implement policies based on previous observations and actions. This non-reactive, history-aware approach is particularly useful in non-Markov decision processes where the same environmental state may lead to different actions depending on prior behavior. As a result, agents can maintain alternative strategies, leading to more flexible and stable behavior that is advantageous in stochastic as well as deterministic environments.\n",
      "\n",
      "4. How Memory Schema is Created: Memory schemas emerge through an evolutionary process. The neuroevolutionary algorithm used in the study relies on mechanisms like neuron duplication and the evolution of recurrent neural network topologies. Through these processes, the network develops the ability for signal reverberation and slow neurodynamic oscillations, thus forming internal memory structures without the need for explicit, pre-programmed memory systems.\n",
      "\n",
      "5. Future Focus Areas on Memory: Future research directions suggested include the introduction of learning during an agent’s life to complement evolutionary development. This would involve incorporating algorithms that allow agents to detect and respond to problems at the neural level while interacting with their environment. There is also potential in integrating insights from theoretical neuroscience to further understand memory mechanisms and enhance adaptive behavior in artificial intelligence systems.\n",
      "\n",
      "6. Conclusion: In summary, memory—specifically short-term memory—plays a crucial role in enabling flexible, goal-directed behavior in adaptive agents. It emerges naturally through evolutionary development in recurrent neural networks via mechanisms such as reverberation and slow neurodynamic processes. This capacity for memory allows agents to use history-dependent strategies, leading to improved performance in complex, stochastic environments. Future work will likely focus on enriching these memory processes through lifetime learning and deeper integration of neural-level mechanisms.\n",
      "neves2023volatilememorymotifsminimal\n",
      "1. Introduction: Memory is a fundamental aspect of both biological and artificial computing systems. In traditional computers, memory is implemented as discrete static components, whereas in self-organizing neural networks memory is stored within the connectivity of neurons and emerges from their collective dynamics. The text emphasizes that memory in neural systems can be volatile, meaning it requires continuous energy (active spiking) to be maintained, distinguishing it from long-term, non-volatile memory.\n",
      "\n",
      "2. Types of Memory: The paper distinguishes between different memory types. It outlines non-volatile memory (such as associative memory in large networks where information is stored in connectivity without ongoing energy input) and volatile memory, which is transient and requires active neuronic activity. The core focus is on a 1-bit volatile memory implemented through minimal network motifs. Additionally, the text briefly mentions multi-dimensional memory, which arises when multiple independent volatile motifs operate in parallel, akin to storing multi-bit words in a traditional computer system.\n",
      "\n",
      "3. How Memory can be Adaptive: Adaptability in memory arises from the parameter-dependent dynamics of the neural circuit motifs. The memory duration (i.e., how long the active state is maintained) can be tuned by varying system parameters such as leakage constants, firing thresholds, delays, and input pulse amplitudes. Transient external signals can switch the network between an 'on' (active spiking, bit 1) and an 'off' (no spiking, bit 0) state. The circuit’s ability to self-terminate its persistent activity through an inhibitory feedback system also illustrates its adaptive nature in response to both internal dynamics and external inputs.\n",
      "\n",
      "4. How Memory Schema is Created: The memory schema is created by a minimal neural network motif composed of two neurons: an excitatory neuron with a delayed self-connection and an inhibitory neuron. The excitatory neuron can continuously generate a spike train (representing a ‘1’ bit) when stimulated by a sufficient external signal, while the absence of spikes represents a ‘0’ bit. The inhibitory neuron, upon accumulating excitatory inputs, eventually triggers a spike that terminates the excitatory signal, thus switching the network state back to ‘0’. This simple yet robust mechanism allows the system to actively hold and then release stored bits of information, and many such motifs can be combined to build a multi-bit memory system.\n",
      "\n",
      "5. Future Focus Areas on Memory: Future research is suggested to explore larger or more complex neural motifs and networks that integrate both local memory function and broader network dynamics. Areas of focus include developing advanced architectures for systematic multi-bit memory storage and reconfiguration, studying the impact of noise and parameter variations on stability, and applying these concepts to neuromorphic and bio-inspired computing systems. Quantitative analyses such as spike counts, leak constants, and timing delays indicate that tuning these parameters can precisely control memory duration and reliability.\n",
      "\n",
      "6. Conclusion: The document concludes that memory in neural systems is an active and dynamic process that is fundamentally different from traditional, static computer memory. By using simple spiking neural circuits, it is possible to implement robust, tunable volatile memory that requires continuous energy input. This approach not only enhances our understanding of biological memory but also paves the way for innovative computing architectures in neuromorphic systems.\n",
      "bai2022saliencyaugmentedmemorycompletioncontinual\n",
      "1. Introduction: In continual learning, 'memory' refers to the storage of previous data samples (or their abstract representations) that help a learning system recall past information and avoid catastrophic forgetting. The approach is inspired by human memory, where only key information is remembered rather than storing every detail. This method seeks to achieve storage efficiency, interpretability, and better generalizability.\n",
      "\n",
      "2. Types of Memory: The text distinguishes between traditional episodic memory, which stores full details of past data samples, and a saliency-augmented memory scheme. In the latter, only the most informative parts of an image (identified via saliency maps such as Grad-CAM) are stored. Additionally, once stored as a sparse representation (using Coordinate Format, COO), the recalled memory is completed (or reconstructed) using methods like rule-based or learning-based (autoencoder) inpainting.\n",
      "\n",
      "3. How Memory can be Adaptive: Memory is made adaptive by selecting only the salient regions of each sample that are most critical for task performance. This selective storage allows the model to adapt to new tasks while keeping only relevant details from old ones. Moreover, a memory completion module inpaints missing parts of the image when needed, enabling the continual learning model to make use of adaptive, restored information. The model parameters of the inpainting module are dynamically updated via a bilevel optimization strategy across tasks, ensuring that memory adapts efficiently as new data arrives.\n",
      "\n",
      "4. How Memory Schema is Created: The memory schema is created by first generating a saliency map for each input image using methods like Grad-CAM. A threshold is applied to this saliency map to extract only the pixels considered important for predictions. The resulting masked image is then stored in episodic memory in a sparse, compressed format (using the COO representation) to optimize storage. When the memory is needed for training on new tasks, an inpainting module (using either rule-based or learning-based approaches) is invoked to reconstruct or 'complete' the missing parts of the image, effectively recreating a full representation for the model to use.\n",
      "\n",
      "5. Future Focus Areas on Memory: Future research could focus on further improving memory efficiency by optimizing the selection and storage of salient information and reducing potential reconstruction errors during memory completion. There is also an open challenge in making the memory not only efficient but highly generalizable and interpretable, thus facilitating better understanding of which features are most crucial. Quantitative results in the paper indicate that the proposed method can achieve about 60% memory savings with only a 7.5% increase in computational cost, suggesting promising avenues for balancing performance with resource efficiency. Further work might explore dynamic adjustment of the stored memory as tasks evolve, enhanced inpainting techniques, and more robust theoretical guarantees for maintaining performance over many tasks.\n",
      "\n",
      "6. Conclusion: The text concludes that incorporating a saliency-augmented memory framework into continual learning systems is highly beneficial. By storing only the key information from previous tasks and reconstructing data when necessary, the system can better manage memory resources while maintaining high accuracy and reducing forgetting. This approach not only yields significant memory savings and improved computational efficiency but also aligns more closely with the human approach to memory, where adaptiveness and generalizability are essential.\n",
      "jitsev2010experiencedrivenformationpartsbasedrepresentations\n",
      "1. Introduction: Memory in this context is the neural substrate that encodes, stores, and retrieves visual stimuli by breaking down complex objects into constituent parts. The model is inspired by evidence from neuroscience that shows that areas such as the inferior temporal cortex, prefrontal cortex, and medial temporal lobe coordinate to form distributed and hierarchical representations. The objective is to form memory traces from repeated exposure to visual objects (e.g., human faces) using parts-based representations.\n",
      "\n",
      "2. Types of Memory: The model distinguishes a two-layer architecture. The lower layer (bunch columns) stores local features (or parts) of the face, such as features around the eyes, nose, and mouth, using bottom-up, lateral, and even forward inhibitory interactions. The higher layer (identity column) represents the global configuration or identity of the face. In addition, the system uses different connectivity types – bottom-up, lateral, and top-down – each with specific roles in forming the final memory trace.\n",
      "\n",
      "3. How Memory can be Adaptive: Memory is adaptive through a process of open-ended, unsupervised learning where the system self-organizes based on experience. Adaptation is achieved via several mechanisms: (a) Bidirectional synaptic plasticity (long-term potentiation and depression) which adjusts the strengths of connections based on correlation of activity levels; (b) Homeostatic activity regulation that adjusts the excitability thresholds of neurons to ensure a balanced use of resources; and (c) Competitive winner-take-all dynamics governed by oscillatory inhibition and excitation, allowing the network to select and amplify the most relevant representations. These mechanisms allow the network to adjust to new input and avoid interference when objects share common parts.\n",
      "\n",
      "4. How Memory Schema is Created: Memory schema formation begins with an undifferentiated connectivity structure. Through repeated, unsupervised exposure to visual stimuli, the network undergoes self-organization. The process includes: (a) Bottom-up learning where local features are captured by receptive fields of the lower layer; (b) Lateral connectivity which builds associative links between different parts that co-occur; (c) Top-down projections that help bind and reinforce the global configuration in an identity unit; and (d) Synaptic scaling and normalization to ensure sparse and differentiated representations. This gradual maturation enables the formation of parts-based representations that cover both local features and global identities in a hierarchical manner.\n",
      "\n",
      "5. Future Focus Areas on Memory: Future research areas suggested include: (a) Exploring transformation-tolerant and invariant processing so that the system can recognize objects under varied conditions; (b) Developing a complete hierarchy that spans from elementary visual features to complex object categories and identities; (c) Integrating mechanisms for active vision and reinforcement learning to adjust memory formation based on behavioral outcomes; and (d) Further investigating the interplay between bottom-up sensory inputs and top-down contextual influences to enhance memory generalization in novel situations.\n",
      "\n",
      "6. Conclusion: The study demonstrates that memory is not a static repository but an adaptive, self-organizing process. Through competitive dynamics, bidirectional plasticity, and homeostatic regulation, the system creates stable, sparse, and distributed representations of visual objects. This adaptive and hierarchical memory framework underscores the importance of recurrent connectivity in achieving rapid learning and robust generalization, offering insights that could be applied to both biological understanding and real-world applications such as machine learning and artificial intelligence.\n",
      "schillaci2020predictionerrordrivenmemoryconsolidation\n",
      "1. Introduction: The text defines memory as a complex system in mammals that comprises multiple memory systems. Particularly, episodic memory is highlighted as crucial for adaptive behaviors such as planning, decision-making, and motor control. The work is inspired by cognitive neuroscience and developmental robotics, and it implements memory consolidation in artificial systems to address continual learning and catastrophic forgetting.\n",
      "\n",
      "2. Types of Memory: The text distinguishes between different memory systems in the brain. It specifically mentions episodic memory and describes the two levels of memory consolidation: a fast, synaptic level (hippocampal) and a slower, more stable level (neocortical). These concepts provide insight into how memories are initially formed and later stabilized.\n",
      "\n",
      "3. How Memory can be Adaptive: Memory is made adaptive by being continuously updated in response to prediction errors—that is, the mismatch between expected and actual outcomes. This mechanism allows the system to retain information that fits well with existing knowledge while updating or reconsolidating parts of memory when unexpected events occur. This process creates a balance between stability (preserving useful past experiences) and plasticity (integrating new information).\n",
      "\n",
      "4. How Memory Schema is Created: In the discussed AI model, memory elements are stored as input-output mappings (for instance, sensor data paired with outcomes such as transpiration and photosynthesis rates). Each memory element is also associated with a prediction error and an expected learning progress, which is calculated as the absolute derivative of the prediction error between successive events. This structured approach allows the system to consolidate memories by evaluating their contribution to future learning, forming a schema that is continuously updated.\n",
      "\n",
      "5. Future Focus Areas on Memory: Future directions suggested include refining prediction error-driven consolidation strategies to better manage the trade-off between stability and plasticity in continual learning. There is also a focus on transferring these memory strategies from controlled laboratory environments to real-world applications (for example, adaptive models for greenhouse climate control) and exploring the impact of heterogeneous datasets on memory consolidation.\n",
      "\n",
      "6. Conclusion: The research concludes that incorporating episodic memory replay and prediction error-driven consolidation in deep recurrent neural networks effectively reduces catastrophic forgetting, balancing stability and plasticity. This brain-inspired mechanism enhances lifelong learning and demonstrates promising results in transferring and adapting research models to practical applications in domains such as greenhouse management.\n",
      "rannentriki2024revisitingdynamicevaluationonline\n",
      "1. Introduction: The concept of memory in the text is related to how large language models store information. Two types of memory are identified: the weight memory (stored in the model parameters via learning) and activation (context) memory (given by the tokens in the attention window). The perspective aligns with memory in neuroscience, where structured processes store, update, and retrieve information.\n",
      "\n",
      "2. Types of Memory: The text distinguishes between two main types of memory in transformer-based language models. One type is memory stored in the model's weights, which is developed and updated gradually through processes such as gradient descent (similar to long-term memory). The other type is the context memory, which is non-parametric and corresponds to the transient hidden states or activations based on the tokens inside the model’s attention window (similar to short-term or working memory).\n",
      "\n",
      "3. How Memory can be Adaptive: The text explains that through online adaptation (or dynamic evaluation), the model parameters become adaptive, turning into a form of temporal, changing state. This allows the model to adjust to distribution shifts and extend its effective context length beyond the fixed window. In effect, the model adapts its weight memory in response to new observations, making it sensitive to changes such as shifts in topic or style.\n",
      "\n",
      "4. How Memory Schema is Created: The memory schema is developed as a result of online adaptation during inference where the parameters are updated via gradient descent. This continuous update mechanism embeds information from recently observed tokens into the model’s weights, thereby creating a temporally evolving memory. This process blurs the lines between classical in-context learning (using activation memory) and fine-tuning (updating weight memory) by effectively integrating new information into the model’s learned representations.\n",
      "\n",
      "5. Future Focus Areas on Memory: The future research avenues mentioned include improving the efficiency of online learning in terms of computational and memory requirements, developing strategies for automatic detection of reset points (to avoid overfitting to local context), and deepening the understanding of the differences between weight memory and activation memory. This could lead to more refined models where the trade-offs between memory in weights and context are better optimized depending on the nature of the distribution shift.\n",
      "\n",
      "6. Conclusion: In summary, the text concludes that dynamic evaluation or online adaptation enhances the model’s performance by continuously updating its memory (through weight adjustments) to capture long-term dependencies beyond the context window. This approach not only improves predictive performance, especially under distribution shifts, but also suggests a new way of conceptualizing memory in large language models, which has important implications for future research and model design.\n",
      "banino2020memodeepnetworkflexible\n",
      "1. Introduction: Memory is described as a system that stores factual or episodic information and is fundamental for inferential reasoning. Inspired by the human hippocampus, which stores episodic memories with pattern separation to avoid interference, memory in neural network architectures is used to store experiences separately for later recombination to infer relationships among data elements.\n",
      "\n",
      "2. Types of Memory: The text highlights two broad types of memory in this context. First, episodic memory consists of distinct, pattern-separated representations of individual events or facts, ensuring minimal interference. Second, integrated or generalized memory emerges at the time of retrieval through mechanisms that combine these separate episodes, enabling inference across multiple memories.\n",
      "\n",
      "3. How Memory can be Adaptive: Memory is made adaptive in these models by employing a variable number of retrieval steps (or hops) before arriving at a final answer. An adaptive retrieval mechanism, using multi-hop attention and a learned halting policy (trained via reinforcement learning), allows the system to decide when sufficient information has been gathered. This process adjusts the computational effort based on the complexity of the task, mirroring adaptive computation time observed in human cognition.\n",
      "\n",
      "4. How Memory Schema is Created: In the discussed architecture (MEMO), the memory schema is built by embedding raw inputs (e.g., words or images) into continuous vector representations. Each input is processed via a linear projection into a common embedding space and stored as distinct items in memory. Instead of fixed positional encodings, a multi-head attention mechanism is used to flexibly weight and recombine these separated facts. This results in a memory structure that retains individual episodes while allowing a flexible recombination during inference.\n",
      "\n",
      "5. Future Focus Areas on Memory: Future research areas include improving the scalability of memory architectures to handle longer sequences and more complex relationships, further reducing interference through more effective pattern separation, and enhancing adaptive retrieval mechanisms to optimize computational resources. There is also interest in integrating these advances within graph reasoning tasks and complex question-answering frameworks, thereby bridging ideas from neuroscience and machine learning.\n",
      "\n",
      "6. Conclusion: The text concludes that memory, as implemented in modern memory-augmented neural networks, is a dynamic and adaptive system. It combines the benefits of episodic storage with sophisticated retrieval strategies that adapt based on task demands. This approach not only emulates certain aspects of human memory but also opens the door to more efficient and robust reasoning in artificial intelligence systems.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import feedparser\n",
    "import re\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "import io\n",
    "import PyPDF2\n",
    "\n",
    "def format_arxiv_query(query):\n",
    "    \"\"\"\n",
    "    Format a query string for the arXiv API to handle multiple words.\n",
    "    Each word is prefixed with \"all:\" and combined with the AND operator.\n",
    "    The resulting string is URL-encoded.\n",
    "    \"\"\"\n",
    "    words = query.strip().split()\n",
    "    if not words:\n",
    "        return \"\"\n",
    "    formatted_query = \" AND \".join(f\"all:{word}\" for word in words)\n",
    "    return urllib.parse.quote(formatted_query)\n",
    "\n",
    "def extract_citation_key(bibtex_text):\n",
    "    \"\"\"\n",
    "    Extracts the citation key from a BibTeX entry string.\n",
    "\n",
    "    Parameters:\n",
    "        bibtex_text (str): The full BibTeX entry as a string.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The citation key if found, otherwise None.\n",
    "    \"\"\"\n",
    "    # This regex matches an entry starting with @type{citation_key,\n",
    "    pattern = r'@\\w+\\s*\\{\\s*([^,]+),'\n",
    "    match = re.search(pattern, bibtex_text)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "async def async_fetch_feed(query_url, session):\n",
    "    \"\"\"\n",
    "    Asynchronously fetch the XML feed from arXiv.\n",
    "    \"\"\"\n",
    "    async with session.get(query_url) as response:\n",
    "        response.raise_for_status()\n",
    "        return await response.text()\n",
    "\n",
    "async def async_get_bibtex_entry(entry, session):\n",
    "    \"\"\"\n",
    "    Asynchronously extract the arXiv id from an entry and retrieve its BibTeX entry.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # entry.id is typically something like \"http://arxiv.org/abs/1234.56789v1\"\n",
    "        abs_url = entry.id\n",
    "        parts = abs_url.split('/abs/')\n",
    "        if len(parts) < 2:\n",
    "            return \"No valid arXiv id found in entry.id\"\n",
    "        arxiv_id_with_version = parts[1]\n",
    "        # Remove version information (e.g., 'v1')\n",
    "        arxiv_id = re.split(\"v\", arxiv_id_with_version)[0]\n",
    "        bibtex_url = f\"https://arxiv.org/bibtex/{arxiv_id}\"\n",
    "        async with session.get(bibtex_url) as response:\n",
    "            response.raise_for_status()\n",
    "            return await response.text()\n",
    "    except Exception as e:\n",
    "        return f\"Error retrieving BibTeX: {str(e)}\"\n",
    "\n",
    "async def async_extract_pdf_text(pdf_url, session):\n",
    "    \"\"\"\n",
    "    Asynchronously retrieve a PDF and extract its text.\n",
    "    Since PDF extraction is blocking, we use asyncio.to_thread.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        async with session.get(pdf_url) as response:\n",
    "            response.raise_for_status()\n",
    "            content = await response.read()\n",
    "            \n",
    "            def extract_text(content):\n",
    "                with io.BytesIO(content) as f:\n",
    "                    reader = PyPDF2.PdfReader(f)\n",
    "                    text = \"\"\n",
    "                    for page in reader.pages:\n",
    "                        page_text = page.extract_text()\n",
    "                        if page_text:\n",
    "                            text += page_text + \"\\n\"\n",
    "                    return text if text else \"No text could be extracted from the PDF.\"\n",
    "            \n",
    "            return await asyncio.to_thread(extract_text, content)\n",
    "    except Exception as e:\n",
    "        return f\"Error retrieving PDF text: {str(e)}\"\n",
    "\n",
    "async def async_extract_html_text(html_url, session):\n",
    "    \"\"\"\n",
    "    Asynchronously retrieve an HTML page and extract its text using BeautifulSoup.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        async with session.get(html_url) as response:\n",
    "            response.raise_for_status()\n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            text = soup.get_text(separator=\"\\n\")\n",
    "            cleaned_text = re.sub(r'\\n+', '\\n', text).strip()\n",
    "            return cleaned_text\n",
    "    except Exception as e:\n",
    "        return f\"Error retrieving HTML text: {str(e)}\"\n",
    "\n",
    "async def async_retrieve_important(user_output: str, metadata_text: str):\n",
    "    '''Retrieve what is important to user_output from metadata_text'''\n",
    "    res = await strict_json_async(f'''From the Text, extract useful information for query: ```{user_output}```\n",
    "You must put all details so that another person can understand without referencing the Text.\n",
    "You must output quantitative results and detailed descriptions whenever applicable.\n",
    "You must output 'NA' if Text is not useful for query or if you are unsure''',\n",
    "                             \"Text: \" + metadata_text[:200000],\n",
    "                             output_format = {\n",
    "                                 \"Text Relevant for query\": \"type: bool\",\n",
    "                                 \"Important Information\": \"type: str\",\n",
    "                                 \"Filtered Detailed Important Information\": f\"Be detailed, only those directly related to query ```{user_output}```, 'NA' if not useful, type: str\"},\n",
    "                             llm = llm)\n",
    "    if res[\"Text Relevant for query\"]:\n",
    "        return res[\"Filtered Detailed Important Information\"]\n",
    "    else:\n",
    "        return 'NA'\n",
    "\n",
    "async def search_arxiv(query, user_output):\n",
    "    \"\"\"\n",
    "    Search arXiv for papers based on a query that is composed of keywords separated by a space and return the top 10 papers with:\n",
    "      - title, abstract, authors, published date\n",
    "      - metadata: text extracted from PDF (if available) or HTML\n",
    "      - BibTeX entry\n",
    "    Uses asynchronous HTTP requests to speed up the retrieval.\n",
    "\n",
    "    user_output is the format we would need the output to be\n",
    "    \"\"\"\n",
    "    base_url = \"http://export.arxiv.org/api/query?\"\n",
    "    formatted_query = format_arxiv_query(query)\n",
    "    query_url = f\"{base_url}search_query={formatted_query}&start=0&max_results=10\"\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Fetch the arXiv feed asynchronously.\n",
    "        xml_data = await async_fetch_feed(query_url, session)\n",
    "        feed = feedparser.parse(xml_data)\n",
    "        \n",
    "        # Schedule tasks for retrieving BibTeX entries and metadata concurrently.\n",
    "        bibtex_tasks = []\n",
    "        metadata_tasks = []\n",
    "        for entry in feed.entries:\n",
    "            # BibTeX task\n",
    "            bibtex_tasks.append(async_get_bibtex_entry(entry, session))\n",
    "            \n",
    "            # Determine PDF and HTML URLs.\n",
    "            pdf_url = None\n",
    "            html_url = None\n",
    "            if hasattr(entry, 'links'):\n",
    "                for link in entry.links:\n",
    "                    if link.get('type') == 'application/pdf':\n",
    "                        pdf_url = link.href\n",
    "                    elif link.get('rel') == 'alternate':\n",
    "                        html_url = link.href\n",
    "            \n",
    "            # Metadata task: try PDF first, then HTML.\n",
    "            if pdf_url:\n",
    "                metadata_tasks.append(async_extract_pdf_text(pdf_url, session))\n",
    "            elif html_url:\n",
    "                metadata_tasks.append(async_extract_html_text(html_url, session))\n",
    "            else:\n",
    "                # If no PDF or HTML link is available, return a default message.\n",
    "                metadata_tasks.append(asyncio.sleep(0, result = entry.summary.strip()))\n",
    "        \n",
    "        # Await all BibTeX and metadata tasks concurrently.\n",
    "        bibtex_entries = await asyncio.gather(*bibtex_tasks)\n",
    "        metadata_texts = await asyncio.gather(*metadata_tasks)\n",
    "\n",
    "        # make bibtex_entries into dict form\n",
    "        bibtex_dict = {extract_citation_key(bibtex_entry): bibtex_entry for bibtex_entry in bibtex_entries}\n",
    "\n",
    "        # Get the important information out\n",
    "        synthesis_tasks = [async_retrieve_important(user_output, metadata_texts[i]) for i in range(len(metadata_texts))]\n",
    "        important_information = await asyncio.gather(*synthesis_tasks)\n",
    "        \n",
    "        return bibtex_dict, important_information\n",
    "\n",
    "user_query = input(\"Enter your search query for arXiv papers: \")\n",
    "user_output = '''What is memory?\n",
    "Required output format:\n",
    "1. Introduction\n",
    "2. Types of Memory\n",
    "3. How Memory can be adaptive\n",
    "4. How Memory schema is created\n",
    "5. Future focus areas on memory\n",
    "6. Conclusion'''\n",
    "bibtex_dict, important_information = await search_arxiv(user_query, user_output)\n",
    "\n",
    "consolidated_info = []\n",
    "if bibtex_dict:\n",
    "    for citationkey, info in zip(bibtex_dict.keys(), important_information):\n",
    "        print(citationkey, info, sep = \"\\n\")\n",
    "        consolidated_info.append({\"citationkey\": citationkey, \"Content\": info})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c11e3b0-05c2-4624-92ba-db6e384a5de1",
   "metadata": {},
   "source": [
    "# Latex Format Display\n",
    "- Install pdflatex and a version of latex on your computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "04a315fe-0a11-4752-9d16-abfe2a46a047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pdflatex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d91279-976d-47ab-8ed6-a76f33ca6526",
   "metadata": {},
   "source": [
    "### Step 1: Create biblatex for references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d403edbf-1d96-418b-b99d-364fab3da19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblatex = '\\n'.join(list(bibtex_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbec9f8e-a8bb-449e-b999-918b9a936843",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"references.bib\", \"w\") as f:\n",
    "    f.write(biblatex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c1cdc5-ff3d-4e81-9e2e-715f95cee48a",
   "metadata": {},
   "source": [
    "### Step 2: Generate main.tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7003619-b5d4-4b3c-814e-d81d9e13a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await strict_json_async(f'''Generate a research report in latex format for the query: ```{user_output}```\n",
    "If format is specified, follow format strictly. Use article format.\n",
    "You must reference whenever possible. Use hyperref and biblatex.\n",
    "Use as many references as possible for each section of the report.\n",
    "Have detailed subsections to highlight the various viewpoints.\n",
    "\n",
    "Print out references at the bottom. Use APA citation.\n",
    "references.bib: ```{biblatex}```''',\n",
    "        consolidated_info,\n",
    "        output_format = {\"Research Report\": \"In latex format, include citations, be as detailed as possible, type: code\"},\n",
    "        llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d21f2a9e-3fbf-4249-a198-1e33aad47b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_latex = res[\"Research Report\"].replace('\\\\\\t', '\\\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa1d9fc7-a00f-42a7-a058-a39aee9e600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"main.tex\", \"w\") as f:\n",
    "    f.write(report_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa09157a-ec7f-45ae-9f16-b7505ece970d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF generated successfully!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Step 1: Run pdflatex to generate aux files.\n",
    "result = subprocess.run(['pdflatex', 'main.tex'], capture_output=True, text=True)\n",
    "if result.returncode != 0:\n",
    "    print(\"Error during initial pdflatex compilation:\")\n",
    "    print(result.stderr)\n",
    "    exit(1)\n",
    "\n",
    "# Step 2: Run bibtex to process the bibliography.\n",
    "# If you're using biblatex with biber, change 'bibtex' to 'biber' and adjust the filename accordingly.\n",
    "result_bib = subprocess.run(['biber', 'main'], capture_output=True, text=True)\n",
    "if result_bib.returncode != 0:\n",
    "    print(\"Error during bibtex compilation:\")\n",
    "    print(result_bib.stderr)\n",
    "    exit(1)\n",
    "\n",
    "# Step 3: Run pdflatex again to resolve references.\n",
    "result = subprocess.run(['pdflatex', 'main.tex'], capture_output=True, text=True)\n",
    "if result.returncode != 0:\n",
    "    print(\"Error during second pdflatex compilation:\")\n",
    "    print(result.stderr)\n",
    "    exit(1)\n",
    "\n",
    "# Optionally run pdflatex one more time if necessary.\n",
    "result = subprocess.run(['pdflatex', 'main.tex'], capture_output=True, text=True)\n",
    "if result.returncode != 0:\n",
    "    print(\"Error during final pdflatex compilation:\")\n",
    "    print(result.stderr)\n",
    "    exit(1)\n",
    "\n",
    "print(\"PDF generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b1c3e9-94e1-4386-95c7-53582ca35340",
   "metadata": {},
   "source": [
    "# Markdown Format Display (alternate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f1be4e9-0b8c-4446-b0a4-bf60932690a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await strict_json_async(f'''Generate a research report in markdown format for the query: ```{user_output}```\n",
    "If format is specified, follow format strictly.\n",
    "You must do in-line citation with the [[1]], [[2]], [[3]] ... whenever possible. \n",
    "Link the citation url in the [[1]], [[2]], [[3]]\n",
    "Use as many sources as possible for each section of the report\n",
    "At the end of the report, list out all the sources using:\n",
    "```[source_number]: APA citation```\n",
    "\n",
    "Citation Details: ```{bibtex_dict}```''',\n",
    "        consolidated_info,\n",
    "        output_format = {\"Research Report\": \"Include citations, be as detailed as possible, type: str\"},\n",
    "        llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1354d932-6609-4d6c-afe4-d84234bd435d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report = res[\"Research Report\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06e141b3-6e57-4fb1-a557-b8238c6d06b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# What is Memory?\n",
       "\n",
       "## 1. Introduction\n",
       "Memory is a fundamental process present in both biological and artificial systems. It encompasses the mechanisms through which information—whether temporal events, spatial features, or sensory inputs—is encoded, stored, and later retrieved [[1]](https://arxiv.org/abs/2412.15292), [[2]](https://arxiv.org/abs/q-bio/0403025), [[8]](https://arxiv.org/abs/2006.12616). In biological systems, memory is intimately tied to the adaptability of neural circuits and learning processes, whilst in artificial intelligence, it informs model architectures that must both store past experiences and dynamically update to meet new contexts [[9]](https://arxiv.org/abs/2403.01518). This report examines these multiple aspects of memory across various systems and proposes potential future research directions.\n",
       "\n",
       "## 2. Types of Memory\n",
       "Memory manifests in several forms and architectures:\n",
       "\n",
       "- **Temporal and Scale Invariant Memory:** Deep reinforcement learning models deploy mechanisms, such as recurrent neural networks (RNNs) and Long Short-Term Memory networks (LSTMs), with innovations like scale invariant (or log-compressed) representations of time, facilitating robust performance across varying temporal conditions [[1]](https://arxiv.org/abs/2412.15292).\n",
       "\n",
       "- **Synaptic Memory in Neural Networks:** Biological systems model memory through synaptic plasticity. Processes like long-term potentiation (LTP) and long-term depression (LTD) enable networks to continuously reshape stored information, offering both stability for frequently used patterns and flexibility through synaptic weakening [[2]](https://arxiv.org/abs/q-bio/0403025), [[7]](https://arxiv.org/abs/0905.2125).\n",
       "\n",
       "- **Volatile vs. Non-volatile Memory:** In self-organizing systems, memory can be volatile—requiring ongoing activity to maintain—or non-volatile, where information is embedded in the network connectivity even after removal of sustained activation [[5]](https://arxiv.org/abs/2303.12225).\n",
       "\n",
       "- **Distributed Memory in Language Models:** Modern language models utilize two kinds of memory: one stored in the model's weights (long-term storage) and another in the transient activation states (context memory), each contributing differently to model performance [[9]](https://arxiv.org/abs/2403.01518).\n",
       "\n",
       "- **Episodic and Integrated Memory:** In memory-augmented architectures, distinct episodic memories store separate, pattern-separated experiences which can later be integrated during inferential reasoning [[10]](https://arxiv.org/abs/2001.10913), as well as frameworks that use saliency-augmented memory for continual learning to prevent catastrophic forgetting [[6]](https://arxiv.org/abs/2212.13242).\n",
       "\n",
       "## 3. How Memory can be Adaptive\n",
       "Adaptivity is central to effective memory systems:\n",
       "\n",
       "- **Scale Invariance:** In computational models, memory is made adaptive by ensuring that the representation of temporal history shifts proportionally when the time scale is altered, maintaining functionality across different environments without the need for readjustment [[1]](https://arxiv.org/abs/2412.15292).\n",
       "\n",
       "- **Synaptic Plasticity and Selective Updating:** Biological networks adapt by modulating synaptic strengths; pathways responsible for errors are downregulated, while those that yield successful responses are reinforced, allowing quick reconfiguration in response to new information [[2]](https://arxiv.org/abs/q-bio/0403025), [[7]](https://arxiv.org/abs/0905.2125).\n",
       "\n",
       "- **Dynamic Adaptation in Oscillator Networks:** Memory associated with potential landscapes can shift in response to external forcing, enabling the network to adapt its stored states dynamically to accommodate new patterns [[3]](https://arxiv.org/abs/2008.07448).\n",
       "\n",
       "- **Online and Continuous Adaptation:** Approaches like dynamic evaluation enable large language models to update their weight memory during inference, thus effectively adapting to distribution shifts and extending their working context [[9]](https://arxiv.org/abs/2403.01518).\n",
       "\n",
       "## 4. How Memory Schema is Created\n",
       "Memory schema creation is a multi-step process across different systems:\n",
       "\n",
       "- **Computational Transformations:** In deep learning, particularly for temporal data, schemas can be derived using mathematical tools such as the Laplace and inverse Laplace transform. This process builds a compressed yet sequential representation of past inputs, mirroring time cells observed in the mammalian brain [[1]](https://arxiv.org/abs/2412.15292).\n",
       "\n",
       "- **Synaptic Organization:** In biological and bio-inspired networks, repetitive exposure to stimuli leads to the self-organization of neural circuits where bottom-up sensory information, lateral connections, and top-down feedback converge to form stable memory representations [[2]](https://arxiv.org/abs/q-bio/0403025), [[7]](https://arxiv.org/abs/0905.2125).\n",
       "\n",
       "- **Potential Landscapes:** In oscillator networks, memory schemas emerge as valleys in amplitude or phase potential landscapes derived from differential equations. These landscapes offer stable attractors corresponding to remembered states, which can be recalled through associative dynamics [[3]](https://arxiv.org/abs/2008.07448).\n",
       "\n",
       "- **Sparse and Salient Representations:** In continual learning models, schemas are constructed by filtering input data through saliency maps, retaining only the most informative features. These features are stored sparsely, and later reconstructed using inpainting methods to alleviate memory storage issues and address catastrophic forgetting [[6]](https://arxiv.org/abs/2212.13242).\n",
       "\n",
       "- **Evolutionary Processes:** Evolutionary mechanisms enable the formation of memory in recurring processes, where neuroevolution helps shape networks that can sustain and recall short-term memories across multiple time steps [[4]](https://arxiv.org/abs/1204.3221).\n",
       "\n",
       "## 5. Future Focus Areas on Memory\n",
       "Future research on memory can be directed along several innovative paths:\n",
       "\n",
       "- **Integration of Dynamic Discounting:** Investigating the integration of scale invariant temporal discounting with adaptive memory architectures to improve learning efficiency and reduce the need for hyperparameter tuning [[1]](https://arxiv.org/abs/2412.15292).\n",
       "\n",
       "- **Optimizing Synaptic Balance:** Further exploration of methods to balance synaptic plasticity (LTP and LTD) to prevent issues like runaway potentiation, ensuring both adaptability and long-term stability in biological and artificial systems [[2]](https://arxiv.org/abs/q-bio/0403025).\n",
       "\n",
       "- **Advanced Neuromorphic Systems:** Extending current methods by exploring global feedback, synchrony in neural oscillator models, and the impact of noise on memory stability could bridge the gap between theoretical models and practical neuromorphic applications [[3]](https://arxiv.org/abs/2008.07448), [[5]](https://arxiv.org/abs/2303.12225).\n",
       "\n",
       "- **Enhanced Online Adaptation:** With large language models becoming ubiquitous, developing more efficient strategies for online adaptation while balancing computational costs and memory efficiency represents a key research challenge [[9]](https://arxiv.org/abs/2403.01518).\n",
       "\n",
       "- **Scalable Memory Architectures:** For memory-augmented networks, future work should aim at scalable architectures that can handle longer sequences, deeper relationships, and multi-modal data while ensuring rapid retrieval and minimal interference [[10]](https://arxiv.org/abs/2001.10913), [[6]](https://arxiv.org/abs/2212.13242).\n",
       "\n",
       "## 6. Conclusion\n",
       "Memory is a dynamic, multifaceted attribute crucial for both biological cognition and artificial intelligence. From synaptic plasticity in biological systems to advanced computational schemas involving Laplace transforms and dynamic evaluation, memory enables systems to store, retrieve, and adapt information for improved decision-making and learning. The convergence of insights from neuroscience, dynamical systems, and machine learning suggests that future advances in memory research could lead to AI systems capable of continual learning and robust performance in changing environments.\n",
       "\n",
       "---\n",
       "\n",
       "### List of Sources\n",
       "\n",
       "[1]: Banerjee, K. et al. (2024). Deep reinforcement learning with time-scale invariant memory. Retrieved from https://arxiv.org/abs/2412.15292\n",
       "\n",
       "[2]: Wakeling, J. R. (2004). Adaptivity and `Per learning. Retrieved from https://arxiv.org/abs/q-bio/0403025\n",
       "\n",
       "[3]: Hoppensteadt, F. (2020). A Frequency-Phase Potential for a Forced STNO Network: an Example of Evoked Memory. Retrieved from https://arxiv.org/abs/2008.07448\n",
       "\n",
       "[4]: Lakhman, K., & Burtsev, M. (2012). Neuroevolution Results in Emergence of Short-Term Memory for Goal-Directed Behavior. Retrieved from https://arxiv.org/abs/1204.3221\n",
       "\n",
       "[5]: Neves, F. S., & Timme, M. (2023). Volatile Memory Motifs: Minimal Spiking Neural Networks. Retrieved from https://arxiv.org/abs/2303.12225\n",
       "\n",
       "[6]: Bai, G., Ling, C., Gao, Y., & Zhao, L. (2022). Saliency-Augmented Memory Completion for Continual Learning. Retrieved from https://arxiv.org/abs/2212.13242\n",
       "\n",
       "[7]: Jitsev, J., & von der Malsburg, C. (2010). Experience-driven formation of parts-based representations in a model of layered visual memory. Retrieved from https://arxiv.org/abs/0905.2125\n",
       "\n",
       "[8]: Schillaci, G., Miranda, L., & Schmidt, U. (2020). Prediction error-driven memory consolidation for continual learning. Retrieved from https://arxiv.org/abs/2006.12616\n",
       "\n",
       "[9]: Rannen-Triki, A., Bornschein, J., Pascanu, R., Hutter, M., György, A., Galashov, A., Teh, Y. W., & Titsias, M. K. (2024). Revisiting Dynamic Evaluation: Online Adaptation for Large Language Models. Retrieved from https://arxiv.org/abs/2403.01518\n",
       "\n",
       "[10]: Banino, A., Puigdomènech Badia, A., Köster, R., Chadwick, M. J., Zambaldi, V., Hassabis, D., Barry, C., Botvinick, M., Kumaran, D., & Blundell, C. (2020). MEMO: A Deep Network for Flexible Combination of Episodic Memories. Retrieved from https://arxiv.org/abs/2001.10913"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(report))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7204790-27aa-484e-bb78-28162dd84139",
   "metadata": {},
   "source": [
    "# Convert Markdown to PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4096cfa2-5850-47c7-9b46-b2b62a7eefb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"main.md\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97bf927-113d-40f2-9659-bede20be171d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
